<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" media="screen" href="/~d/styles/atom10full.xsl"?><?xml-stylesheet type="text/css" media="screen" href="http://feeds.feedburner.com/~d/styles/itemcontent.css"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:feedburner="http://rssnamespace.org/feedburner/ext/1.0"><title>JBoss Tools Aggregated Feed</title><link rel="alternate" href="http://tools.jboss.org" /><subtitle>JBoss Tools Aggregated Feed</subtitle><dc:creator>JBoss Tools</dc:creator><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="self" type="application/atom+xml" href="http://feeds.feedburner.com/jbossbuzz" /><feedburner:info uri="jbossbuzz" /><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="hub" href="http://pubsubhubbub.appspot.com/" /><entry><title>Patterns for distributed transactions within a microservices architecture</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/GhWENUizMsY/" /><category term="Microservices" /><category term="Modern App Dev" /><category term="2pc" /><category term="App Dev" /><category term="architecture" /><category term="Design Patterns" /><category term="enterprise architecture" /><category term="microservices" /><category term="pattern" /><category term="Saga pattern" /><category term="two-phase commit" /><author><name>Keyang Xiang</name></author><id>https://developers.redhat.com/blog/?p=520997</id><updated>2018-10-01T20:05:00Z</updated><published>2018-10-01T20:05:00Z</published><content type="html">&lt;p&gt;&lt;img class=" alignright wp-image-522177 " data-add-featherlight="https://developers.redhat.com/blog/wp-content/uploads/2018/09/APP_DEV_CoE_reverse-1.png" src="https://developers.redhat.com/blog/wp-content/uploads/2018/09/APP_DEV_CoE_reverse-1.png" alt="Red Hat Application Development Center of Excellence Logo" width="322" height="110" srcset="https://developers.redhat.com/blog/wp-content/uploads/2018/09/APP_DEV_CoE_reverse-1.png 322w, https://developers.redhat.com/blog/wp-content/uploads/2018/09/APP_DEV_CoE_reverse-1-300x102.png 300w" sizes="(max-width: 322px) 100vw, 322px" /&gt;&lt;/p&gt; &lt;p&gt;&lt;a href="https://developers.redhat.com/topics/microservices/"&gt;Microservices&lt;/a&gt; architecture (MSA) has become very popular.. However, one common problem is how to manage distributed transactions across multiple microservices. This post is going to share my experience from past projects and explain the problem and possible patterns that could solve it.&lt;/p&gt; &lt;h2&gt;What is a distributed transaction?&lt;/h2&gt; &lt;p&gt;When a microservice architecture decomposes a monolithic system into self-encapsulated services, it can break transactions. This means a &lt;strong&gt;local transaction &lt;/strong&gt;in the monolithic system is now &lt;strong&gt;distributed &lt;/strong&gt;into multiple services that will be called in a sequence.&lt;span id="more-520997"&gt;&lt;/span&gt;&lt;/p&gt; &lt;p&gt;Here is a customer order example with a monolithic system using a local transaction:&lt;/p&gt; &lt;p&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2018/09/Untitled-UML-4.png"&gt;&lt;img class=" alignright wp-image-521067 " data-add-featherlight="https://developers.redhat.com/blog/wp-content/uploads/2018/09/Untitled-UML-4.png" src="https://developers.redhat.com/blog/wp-content/uploads/2018/09/Untitled-UML-4.png" alt="Diagram of customer order example with a monolithic system using a local transaction" width="475" height="551" srcset="https://developers.redhat.com/blog/wp-content/uploads/2018/09/Untitled-UML-4.png 475w, https://developers.redhat.com/blog/wp-content/uploads/2018/09/Untitled-UML-4-259x300.png 259w" sizes="(max-width: 475px) 100vw, 475px" /&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;In the customer order example above, if a user sends a &lt;strong&gt;Put Order &lt;/strong&gt;action to a monolithic system, the system will create a  local database transaction that works over multiple database tables. If any step fails, the transaction can &lt;strong&gt;roll back&lt;/strong&gt;. This is known as ACID (Atomicity, Consistency, Isolation, Durability), which is guaranteed by the database system.&lt;/p&gt; &lt;p&gt;When we decompose this system, we created both the &lt;code&gt;CustomerMicroservice&lt;/code&gt;and the &lt;code&gt;OrderMicroservice&lt;/code&gt;&lt;strong&gt;,&lt;/strong&gt; which have separate databases. Here is a customer order example with microservices:&lt;/p&gt; &lt;p&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2018/09/Untitled-UML-5.png"&gt;&lt;img class=" alignright wp-image-521087 " data-add-featherlight="https://developers.redhat.com/blog/wp-content/uploads/2018/09/Untitled-UML-5.png" src="https://developers.redhat.com/blog/wp-content/uploads/2018/09/Untitled-UML-5.png" alt="Diagram of customer order example with microservices" width="783" height="463" srcset="https://developers.redhat.com/blog/wp-content/uploads/2018/09/Untitled-UML-5.png 783w, https://developers.redhat.com/blog/wp-content/uploads/2018/09/Untitled-UML-5-300x177.png 300w, https://developers.redhat.com/blog/wp-content/uploads/2018/09/Untitled-UML-5-768x454.png 768w" sizes="(max-width: 783px) 100vw, 783px" /&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;When a &lt;strong&gt;Put Order&lt;/strong&gt; request comes from the user, both microservices will be called to apply changes into their own database. Because the transaction is now across multiple databases, it is now considered a &lt;strong&gt;distributed transaction&lt;/strong&gt;.&lt;/p&gt; &lt;h2&gt;What is the problem?&lt;/h2&gt; &lt;p&gt;In a monolithic system, we have a database system to ensure ACIDity. We now need to clarify the following key problems.&lt;/p&gt; &lt;h3&gt;&lt;strong&gt;How do we keep the transaction atomic?&lt;/strong&gt;&lt;/h3&gt; &lt;p&gt;In a database system, atomicity means that in a transaction either &lt;strong&gt;all steps complete&lt;/strong&gt; or &lt;strong&gt;no steps complete. &lt;/strong&gt;The microservice-based system does not have a global transaction coordinator by default. In the example above, if the &lt;code&gt;CreateOrder&lt;/code&gt; method fails, how do we roll back the changes we applied by the &lt;code&gt;CustomerMicroservice&lt;/code&gt;&lt;strong&gt;? &lt;/strong&gt;&lt;/p&gt; &lt;h3&gt;&lt;strong&gt;Do we isolate user actions for concurrent requests?&lt;/strong&gt;&lt;/h3&gt; &lt;p&gt;If an object is written by a transaction and at the same time (before the transaction ends), it is read by another request, should the object return old data or updated data? In the example above, once &lt;code&gt;UpdateCustomerFund&lt;/code&gt; succeeds but is still waiting for a response from &lt;code&gt;CreateOrder&lt;/code&gt;, should requests for the current customer&amp;#8217;s fund return the updated amount or not?&lt;/p&gt; &lt;h2&gt;Possible solutions&lt;/h2&gt; &lt;p&gt;The problems above are important for microservice-based systems. Otherwise, there is no way to tell if a transaction has completed successfully. The following two patterns can resolve the problem:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;2pc (two-phase commit)&lt;/li&gt; &lt;li&gt;Saga&lt;/li&gt; &lt;/ul&gt; &lt;h3&gt;Two-phase commit (2pc) pattern&lt;/h3&gt; &lt;p&gt;2pc is widely used in database systems. For some situations, you can use 2pc for microservices. Just be careful; not all situations suit 2pc and, in fact, 2pc is considered impractical within a microservice architecture (explained below).&lt;/p&gt; &lt;p&gt;So what is a two-phase commit?&lt;/p&gt; &lt;p&gt;As its name hints, 2pc has two phases: A &lt;em&gt;prepare phase&lt;/em&gt; and a &lt;em&gt;commit phase&lt;/em&gt;. In the prepare phase, all microservices will be asked to prepare for some data change that could be done atomically. Once all microservices are prepared, the commit phase will ask all the microservices to make the actual changes.&lt;/p&gt; &lt;p&gt;Normally, there needs to be a global coordinator to maintain the lifecycle of the transaction, and the coordinator will need to call the microservices in the prepare and commit phases.&lt;/p&gt; &lt;p&gt;Here is a 2pc implementation for the customer order example:&lt;/p&gt; &lt;p&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2018/09/Untitled-UML-6.png"&gt;&lt;img class=" alignright wp-image-521477 " data-add-featherlight="https://developers.redhat.com/blog/wp-content/uploads/2018/09/Untitled-UML-6.png" src="https://developers.redhat.com/blog/wp-content/uploads/2018/09/Untitled-UML-6.png" alt="Diagram of 2pc implementation for the customer order example" width="544" height="414" srcset="https://developers.redhat.com/blog/wp-content/uploads/2018/09/Untitled-UML-6.png 544w, https://developers.redhat.com/blog/wp-content/uploads/2018/09/Untitled-UML-6-300x228.png 300w" sizes="(max-width: 544px) 100vw, 544px" /&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;In the example above, when a user sends a put order request, the &lt;code&gt;Coordinator&lt;/code&gt; will first create a global transaction with all the context information. It will then tell &lt;code&gt;CustomerMicroservice&lt;/code&gt; to prepare for updating a customer fund with the created transaction. The &lt;code&gt;CustomerMicroservice&lt;/code&gt; will then check, for example, if the customer has enough funds to proceed with the transaction. Once &lt;code&gt;CustomerMicroservice&lt;/code&gt; is OK to perform the change, it will lock down the object from further changes and tell the &lt;code&gt;Coordinator&lt;/code&gt; that it is prepared. The same thing happens while creating the order in the &lt;code&gt;OrderMicroservice&lt;/code&gt;. Once the &lt;code&gt;Coordinator&lt;/code&gt; has confirmed all microservices are ready to apply their changes, it will then ask them to apply their changes by requesting a commit with the transaction. At this point, all objects will be unlocked.&lt;/p&gt; &lt;p&gt;If at any point a single microservice fails to prepare, the &lt;code&gt;Coordinator&lt;/code&gt; will abort the transaction and begin the rollback process. Here is a diagram of a 2pc rollback for the customer order example:&lt;/p&gt; &lt;p&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2018/09/Untitled-UML-7.png"&gt;&lt;img class=" alignright wp-image-521497 " data-add-featherlight="https://developers.redhat.com/blog/wp-content/uploads/2018/09/Untitled-UML-7.png" src="https://developers.redhat.com/blog/wp-content/uploads/2018/09/Untitled-UML-7.png" alt="Diagram of a 2pc rollback for the customer order example" width="543" height="356" srcset="https://developers.redhat.com/blog/wp-content/uploads/2018/09/Untitled-UML-7.png 543w, https://developers.redhat.com/blog/wp-content/uploads/2018/09/Untitled-UML-7-300x197.png 300w" sizes="(max-width: 543px) 100vw, 543px" /&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;In the above example, the &lt;code&gt;CustomerMicroservice&lt;/code&gt; failed to prepare for some reason, but the &lt;code&gt;OrderMicroservice&lt;/code&gt; has replied that it is prepared to create the order. The &lt;code&gt;Coordinator&lt;/code&gt; will request an abort on the &lt;code&gt;OrderMicroservice&lt;/code&gt; with the transaction and the &lt;code&gt;OrderMicroservice&lt;/code&gt; will then roll back any changes made and unlock the database objects.&lt;/p&gt; &lt;h4&gt;Benefits of using 2pc&lt;/h4&gt; &lt;p&gt;2pc is a very strong consistency protocol. First, the prepare and commit phases guarantee that the transaction is atomic. The transaction will end with either all microservices returning successfully or all microservices have nothing changed.  Secondly, 2pc allows read-write isolation. This means the changes on a field are not visible until the coordinator commits the changes.&lt;/p&gt; &lt;h4&gt;Disadvantages of using 2pc&lt;/h4&gt; &lt;p&gt;While 2pc has solved the problem, it is not really recommended for many microservice-based systems because 2pc is synchronous (blocking). The protocol will need to lock the object that will be changed before the transaction completes. In the example above, if a customer places an order, the &amp;#8220;fund&amp;#8221; field will be locked for the customer. This prevents the customer from applying new orders. This makes sense because if a &amp;#8220;prepared&amp;#8221; object changed after it claims it is &amp;#8220;prepared,&amp;#8221; then the commit phase could possibly not work.&lt;/p&gt; &lt;p&gt;This is not good. In a database system, transactions tend to be fast—normally within 50 ms. However, microservices have long delays with RPC calls, especially when integrating with external services such as a payment service. The lock could become a system performance bottleneck. Also, it is possible to have two transactions mutually lock each other (deadlock) when each transaction requests a lock on a resource the other requires.&lt;/p&gt; &lt;h3&gt;Saga pattern&lt;/h3&gt; &lt;p&gt;The Saga pattern is another widely used pattern for distributed transactions. It is different from 2pc, which is synchronous. The Saga pattern is asynchronous and reactive. In a Saga pattern, the distributed transaction is fulfilled by asynchronous local transactions on all related microservices. The microservices communicate with each other through an event bus.&lt;/p&gt; &lt;p&gt;Here is a diagram of the Saga pattern for the customer order example:&lt;/p&gt; &lt;p&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2018/09/Untitled-UML-8.png"&gt;&lt;img class=" alignright wp-image-521807 " data-add-featherlight="https://developers.redhat.com/blog/wp-content/uploads/2018/09/Untitled-UML-8.png" src="https://developers.redhat.com/blog/wp-content/uploads/2018/09/Untitled-UML-8.png" alt="Diagram of the Saga pattern for the customer order example" width="473" height="283" srcset="https://developers.redhat.com/blog/wp-content/uploads/2018/09/Untitled-UML-8.png 473w, https://developers.redhat.com/blog/wp-content/uploads/2018/09/Untitled-UML-8-300x179.png 300w" sizes="(max-width: 473px) 100vw, 473px" /&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;In the example above, the &lt;code&gt;OrderMicroservice&lt;/code&gt; receives a request to place an order. It first starts a local transaction to create an order and then emits an &lt;code&gt;OrderCreated&lt;/code&gt; event. The &lt;code&gt;CustomerMicroservice&lt;/code&gt; listens for this event and updates a customer fund once the event is received. If a deduction is successfully made from a fund, a &lt;code&gt;CustomerFundUpdated&lt;/code&gt; event will then be emitted, which in this example means the end of the transaction.&lt;/p&gt; &lt;p&gt;If any microservice fails to complete its local transaction, the other microservices will run compensation transactions to rollback the changes. Here is a diagram of the Saga pattern for a compensation transaction:&lt;/p&gt; &lt;p&gt;&lt;a href="https://developers.redhat.com/blog/wp-content/uploads/2018/09/Untitled-UML-9.png"&gt;&lt;img class=" alignright wp-image-521817 " data-add-featherlight="https://developers.redhat.com/blog/wp-content/uploads/2018/09/Untitled-UML-9.png" src="https://developers.redhat.com/blog/wp-content/uploads/2018/09/Untitled-UML-9.png" alt="Diagram of the Saga pattern for a compensation transaction" width="651" height="365" srcset="https://developers.redhat.com/blog/wp-content/uploads/2018/09/Untitled-UML-9.png 651w, https://developers.redhat.com/blog/wp-content/uploads/2018/09/Untitled-UML-9-300x168.png 300w" sizes="(max-width: 651px) 100vw, 651px" /&gt;&lt;/a&gt;&lt;/p&gt; &lt;p&gt;In the above example, the &lt;code&gt;UpdateCustomerFund&lt;/code&gt; failed for some reason and it then emitted a &lt;code&gt;CustomerFundUpdateFailed&lt;/code&gt; event. The &lt;code&gt;OrderMicroservice&lt;/code&gt; listens for the event and start its compensation transaction to revert the order that was created.&lt;/p&gt; &lt;h4&gt;Advantages of the Saga pattern&lt;/h4&gt; &lt;p&gt;One big advantage of the Saga pattern is its support for long-lived transactions. Because each microservice focuses only on its own local atomic transaction, other microservices are not blocked if a microservice is running for a long time. This also allows transactions to continue waiting for user input. Also, because all local transactions are happening in parallel, there is no lock on any object.&lt;/p&gt; &lt;h4&gt;Disadvantages of the Saga pattern&lt;/h4&gt; &lt;p&gt;The Saga pattern is difficult to debug, especially when many microservices are involved. Also, the event messages could become difficult to maintain if the system gets complex. Another disadvantage of the Saga pattern is it does not have read isolation. For example, the customer could see the order being created, but in the next second, the order is removed due to a compensation transaction.&lt;/p&gt; &lt;h4&gt;Adding a process manager&lt;/h4&gt; &lt;p&gt;To address the complexity issue of the Saga pattern, it is quite normal to add a process manager as an orchestrator. The process manager is responsible for listening to events and triggering endpoints.&lt;/p&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;The Saga pattern is a preferable way of solving distributed transaction problems for a microservice-based architecture. However, it also introduces a new set of problems, such as how to atomically update the database and emit an event. Adoption of the Saga pattern requires a change in mindset for both development and testing. It could be a challenge for a team that is not familiar with this pattern. There are many variants that simplify its implementation. Therefore, it is important to choose the proper way to implement it for a project.&lt;/p&gt; &lt;p&gt;&lt;a class="a2a_button_facebook" href="https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F10%2F01%2Fpatterns-for-distributed-transactions-within-a-microservices-architecture%2F&amp;#38;linkname=Patterns%20for%20distributed%20transactions%20within%20a%20microservices%20architecture" title="Facebook" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_twitter" href="https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F10%2F01%2Fpatterns-for-distributed-transactions-within-a-microservices-architecture%2F&amp;#38;linkname=Patterns%20for%20distributed%20transactions%20within%20a%20microservices%20architecture" title="Twitter" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_google_plus" href="https://www.addtoany.com/add_to/google_plus?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F10%2F01%2Fpatterns-for-distributed-transactions-within-a-microservices-architecture%2F&amp;#38;linkname=Patterns%20for%20distributed%20transactions%20within%20a%20microservices%20architecture" title="Google+" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_linkedin" href="https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F10%2F01%2Fpatterns-for-distributed-transactions-within-a-microservices-architecture%2F&amp;#38;linkname=Patterns%20for%20distributed%20transactions%20within%20a%20microservices%20architecture" title="LinkedIn" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_email" href="https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F10%2F01%2Fpatterns-for-distributed-transactions-within-a-microservices-architecture%2F&amp;#38;linkname=Patterns%20for%20distributed%20transactions%20within%20a%20microservices%20architecture" title="Email" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_print" href="https://www.addtoany.com/add_to/print?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F10%2F01%2Fpatterns-for-distributed-transactions-within-a-microservices-architecture%2F&amp;#38;linkname=Patterns%20for%20distributed%20transactions%20within%20a%20microservices%20architecture" title="Print" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_reddit" href="https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F10%2F01%2Fpatterns-for-distributed-transactions-within-a-microservices-architecture%2F&amp;#38;linkname=Patterns%20for%20distributed%20transactions%20within%20a%20microservices%20architecture" title="Reddit" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_flipboard" href="https://www.addtoany.com/add_to/flipboard?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F10%2F01%2Fpatterns-for-distributed-transactions-within-a-microservices-architecture%2F&amp;#38;linkname=Patterns%20for%20distributed%20transactions%20within%20a%20microservices%20architecture" title="Flipboard" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_dd addtoany_share_save addtoany_share" href="https://www.addtoany.com/share#url=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F10%2F01%2Fpatterns-for-distributed-transactions-within-a-microservices-architecture%2F&amp;#38;title=Patterns%20for%20distributed%20transactions%20within%20a%20microservices%20architecture" data-a2a-url="https://developers.redhat.com/blog/2018/10/01/patterns-for-distributed-transactions-within-a-microservices-architecture/" data-a2a-title="Patterns for distributed transactions within a microservices architecture"&gt;&lt;img src="https://static.addtoany.com/buttons/favicon.png" alt="Share"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2018/10/01/patterns-for-distributed-transactions-within-a-microservices-architecture/"&gt;Patterns for distributed transactions within a microservices architecture&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;RHD Blog&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/GhWENUizMsY" height="1" width="1" alt=""/&gt;</content><summary type="html">&lt;p&gt;Microservices architecture (MSA) has become very popular.. However, one common problem is how to manage distributed transactions across multiple microservices. This post is going to share my experience from past projects and explain the problem and possible patterns that could solve it. What is a distributed transaction? When a microservice architecture decomposes a monolithic system into self-encapsulated [&amp;#8230;]&lt;/p&gt; &lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2018/10/01/patterns-for-distributed-transactions-within-a-microservices-architecture/"&gt;Patterns for distributed transactions within a microservices architecture&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;RHD Blog&lt;/a&gt;.&lt;/p&gt;</summary><wfw:commentRss xmlns:wfw="http://wellformedweb.org/CommentAPI/">https://developers.redhat.com/blog/2018/10/01/patterns-for-distributed-transactions-within-a-microservices-architecture/feed/</wfw:commentRss><slash:comments xmlns:slash="http://purl.org/rss/1.0/modules/slash/">0</slash:comments><post-id xmlns="com-wordpress:feed-additions:1">520997</post-id><dc:creator>Keyang Xiang</dc:creator><dc:date>2018-10-01T20:05:00Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2018/10/01/patterns-for-distributed-transactions-within-a-microservices-architecture/</feedburner:origLink></entry><entry><title>The Cathedral and the Bazaar: Moving from Barter to a Currency System</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/SBHmDWygm6Q/the-cathedral-and-bazaar-moving-from.html" /><category term="Blockchain" scheme="searchisko:content:tags" /><category term="feed_group_name_fusesource" scheme="searchisko:content:tags" /><category term="feed_name_ofbizian" scheme="searchisko:content:tags" /><category term="open source" scheme="searchisko:content:tags" /><author><name>Bilgin Ibryam</name></author><id>searchisko:content:id:jbossorg_blog-the_cathedral_and_the_bazaar_moving_from_barter_to_a_currency_system</id><updated>2018-10-01T11:46:59Z</updated><published>2018-10-01T11:43:00Z</published><content type="html">&lt;span style="font-family: inherit;"&gt;&lt;i&gt;This post was originally published as "How blockchain can complement open source" on &lt;a href="https://opensource.com/article/18/9/barter-currency-system" target="_blank"&gt;Opensource.com&lt;/a&gt; under CC BY-SA 4.0. &lt;/i&gt;&lt;/span&gt;&lt;i&gt;If you prefer, you can also read the same post on &lt;a href="https://medium.com/@bibryam/the-cathedral-and-the-bazaar-moving-from-barter-to-a-currency-system-4f83c0fd8bb4" target="_blank"&gt;Medium&lt;/a&gt;. &lt;/i&gt;&lt;br /&gt;&lt;h3&gt;Open Won Over Closed&lt;/h3&gt;&lt;a href="http://catb.org/"&gt;The Cathedral and The Bazaar&lt;/a&gt; is the classic open source story written 20 years ago by Eric Steven Raymond. In the story, Eric describes a new revolutionary software development model where complex software projects are built without (or with a very little) central management. This new model is open source. Eric's story compares two models:&lt;br /&gt;&lt;ul&gt;&lt;li&gt;The classic model (represented by the cathedral) where software is crafted by a small group of individuals in a closed and controlled environment through slow and stable releases.&lt;/li&gt;&lt;li&gt;And the new model (represented by the bazaar) where software is crafted in an open environment where individuals can participate freely, but still produce a stable and coherent system.&lt;/li&gt;&lt;/ul&gt;Some of the reasons for open source being so successful can be traced back to the founding principles described by Eric. Releasing early, releasing often, accepting the fact that many heads are inevitably better than one allows open source projects to tap into the world's pool of talent (and not many companies can match that using the closed source model).&lt;br /&gt;&lt;br /&gt;Two decades after Eric's reflective analysis of the hacker community, we see open source becoming dominant. It is not any longer a model only for scratching a developer’s personal itch, but instead, the place where innovation happens. It is the model that even worlds&lt;a href="http://oss.cash/"&gt; largest&lt;/a&gt; software companies are transitioning to in order to continue dominating.&lt;br /&gt;&lt;h3&gt;A Barter System&lt;/h3&gt;If we look closely at how the open source model works in practice, we realize that it is a closed system exclusive only to open source developers and techies. The only way to influence the direction of a project is by joining the open source community, understanding the written and the unwritten rules, learning how to contribute, the coding standards, etc, and doing it yourself. This is how the bazaar works and where the barter system analogy comes from. A barter system is a method of exchange of services and goods for other services and goods in return. In the bazaar - where the software is built, that means, in order to take something, you have to be also a producer yourself, and give something back in return. And that is, by exchanging your time and knowledge for getting something done. A bazaar is a place where open source developers interact with other open source developers and produce open source software, the open source way.&lt;br /&gt;&lt;br /&gt;The barter system is a great step forward and an evolution from the state of self-sufficiency where everybody has to be a jack of all trades. The bazaar (open source model) using the barter system allows people with common interests and different skills to gather, collaborate and create something that no individual can create on their own. The barter system is simple and lacks complex problems of the modern monetary systems, but it also has some limitations to name a few:&lt;br /&gt;&lt;ul&gt;&lt;li&gt;Lack of divisibility - in the absence of a common medium of exchange, a large indivisible commodity/value cannot be exchanged for a smaller commodity/value. For example, even if you want to do a small change in an open source project, you may still have to go through a high entry barrier sometimes.&lt;/li&gt;&lt;li&gt;Storing value - if a project is important to your company, you may want to have a large investment/commitment in it. But since it is a barter system among open source developers, the only way to have a strong say is by employing many open source committers and that is not always possible.&lt;/li&gt;&lt;li&gt;Transferring value - if you have invested in a project (trained employees, hired open source developers) and want to move focus to another project, it is not possible to transfer expertise, reputation, influence quickly.&lt;/li&gt;&lt;li&gt;Temporal decoupling - the barter system does not provide a good mechanism for deferred or in advance commitments. In the open source world, that means a user cannot express its commitment/interest in a project in a measurable way in advance, or continuously for future periods.&lt;/li&gt;&lt;/ul&gt;We will see below what is the back door to the bazaar and how to address these limitations.&lt;br /&gt;&lt;h3&gt;A Currency System&lt;/h3&gt;People are hanging at the bazaar for different reasons: some are there to learn, some are there to scratch a developer’s personal itch and some work for large software farms. And since the only way to have a say in the bazaar is by becoming part of the open source community and joining the barter system, in order to gain credibility in the open source world, many large software companies pay these developers in a monetary value and employ them. The latter represents the use of a currency system to influence the bazaar. Open source is not any longer for scratching the personal developer itch only. It also accounts for a significant part of the overall software production worldwide and there are many who want to have an influence.&lt;br /&gt;&lt;br /&gt;Open source sets the guiding principles through which developers interact and build a coherent system in a distributed way. It dictates how a project is governed, software is built and the output distributed to users. It is an open consensus model for decentralized entities for building quality software together. But the open source model does not cover how open source is subsidized. Whether it is sponsored, directly or indirectly, through intrinsic or extrinsic motivators is irrelevant to the bazaar.&lt;br /&gt;&lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto; text-align: center;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://1.bp.blogspot.com/-5_yDt3OqI3Y/W4LtWzFkPXI/AAAAAAAALWY/nGTjXySgmEsc8ZEUvb8MkdGzjHsXiXNugCLcBGAs/s1600/Tokenomics%2B-%2BPage%2B4%25281%2529.png" imageanchor="1" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="1193" data-original-width="1600" height="475" src="https://1.bp.blogspot.com/-5_yDt3OqI3Y/W4LtWzFkPXI/AAAAAAAALWY/nGTjXySgmEsc8ZEUvb8MkdGzjHsXiXNugCLcBGAs/s640/Tokenomics%2B-%2BPage%2B4%25281%2529.png" width="640" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;&lt;div dir="ltr" id="docs-internal-guid-19ce7356-7fff-5fe9-f057-4b2c3af0d568" style="line-height: 1.38; margin-bottom: 0pt; margin-top: 0pt; text-align: center;"&gt;&lt;i&gt;Centralized and decentralized ecosystems supporting open source&lt;/i&gt;&lt;/div&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;br /&gt;Currently, there is no equivalent of the decentralized open source development model for subsidization purpose. The majority of the open source subsidization is centralized and monopolized typically by one company which dominates a project by employing the majority of the open source developers of that project. And to be honest, this is currently the best case scenario which guarantees that the developers will be employed and the project will continue flourishing. While a company is working on its paid software or services whether that is SaaS subsodizes open source development indirectly.&lt;br /&gt;&lt;br /&gt;There are also exceptions for the project monopoly scenario: for example, some of the Cloud Native Computing Foundation projects are developed by a large number of competing companies. Also, the Apache Software Foundation aims for their projects not to be dominated by a single vendor by encouraging diverse contributors, but most of the popular projects, in reality, are still single vendor projects...&lt;br /&gt;&lt;br /&gt;What we are still missing is an open and decentralized model that works like the bazaar without a central coordination and ownership, where consumers (open source users) and producers (open source developers) interact with each other driven by market forces and open source value. In order to complement open source, such a model also has to be open and decentralized and this is why the blockchain technology would&lt;a href="https://opensource.com/article/18/8/open-source-tokenomics"&gt; fit here best&lt;/a&gt;. This would create an complementary ecosystem that flows subsidies from users to developers, but without a centralized and monopolized entity (such as an open source company). There are already successful open source cryptocurrency projects such as&lt;a href="https://www.decred.org/"&gt; Decred&lt;/a&gt;,&lt;a href="https://www.dash.org/"&gt; Dash&lt;/a&gt;,&lt;a href="https://getmonero.org/"&gt; Monero&lt;/a&gt;,&lt;a href="https://z.cash/"&gt; Zcash&lt;/a&gt;, that use a similar&lt;a href="https://nadiaeghbal.com/grant-programs"&gt; decentralized funding model&lt;/a&gt; where a portion of the mining or donations subsidy is used for their own development.&lt;br /&gt;&lt;br /&gt;Most of the existing platforms (blockchain or non-blockchain) that aim to subsidize open source development are targeting primarily bug bounties, small and piecemeal tasks. There are also a few focused on the funding of new open source projects. But there are not many that aim to provide mechanisms for sustaining continued development of open source projects. Basically, a system that would emulate the behavior of an open source service provider company, or open core, open source based SaaS product company: ensuring developers get continued and predictable incentives, and guiding the project development based on the priorities of the incentivizers, i.e. the users. Such a model would address the limitations of the barter system listed above:&lt;br /&gt;&lt;ul&gt;&lt;li&gt;Allow divisibility - if you want something small fixed, you can pay a small amount without paying the full premium of becoming an open source developer for a project.&lt;/li&gt;&lt;li&gt;Storing value - you can invest a large amount into a project and ensure its continued development and ensure your voice is heard.&lt;/li&gt;&lt;li&gt;Transferring value - at any point, you can stop investing in the project and move funds into other projects.&lt;/li&gt;&lt;li&gt;Temporal decoupling - allow regular recurring payments and subscriptions.&lt;/li&gt;&lt;/ul&gt;There would be also other benefits raising purely from the fact that such a blockchain based system is transparent and decentralized: to quantify a project's value/usefulness based on its users' commitment, decentralized roadmap governance, decentralized decision making, etc. While there still will be user who prefer to use the more centrally managed software, there will be others who prefer the more transparent and decentralized way of influencing projects. There is enough room for all parties.&lt;br /&gt;&lt;h3&gt;Conclusion&lt;/h3&gt;On the one hand, we see large companies hiring open source developers, and acquiring open source startups and even foundational platforms (such as Microsoft buying Github). Many if not most long-running successful open source projects are centralised around a single vendor. The significance of open source, and its centralisation is a fact.&lt;br /&gt;&lt;br /&gt;On the other hand, the challenges around&lt;a href="https://www.youtube.com/watch?v=VS6IpvTWwkQ"&gt; sustaining open source&lt;/a&gt; software are becoming more apparent, and there are many investigating deeper this space and its foundational issues. There are a few projects with high visibility and a large number of contributors, but there are also many other still important projects but with not enough contributors and maintainers.&lt;br /&gt;&lt;br /&gt;There are&lt;a href="https://opensource.com/article/18/8/open-source-tokenomics"&gt; many efforts&lt;/a&gt; trying to address the challenges of open source through blockchain. These projects should improve the transparency, decentralization, subsidization, and establish a direct link between open source users and developers. This space is still very young but progressing fast, and with time, the bazaar is going to have a cryptocurrency system.&lt;br /&gt;&lt;br /&gt;Given enough time, and adequate technology, decentralization is happening at many levels:&lt;br /&gt;&lt;ul&gt;&lt;li&gt;The Internet is a decentralised medium that unlocked world's potential for sharing and acquiring knowledge.&lt;/li&gt;&lt;li&gt;Open source is a decentralized collaboration model that unlocked the world's potential for innovation.&lt;/li&gt;&lt;li&gt;And similarly, blockchain can complement open source and become the decentralized open source subsidization model.&lt;/li&gt;&lt;/ul&gt;Follow me on &lt;a href="http://twitter.com/bibryam"&gt;twitter&lt;/a&gt; for other posts in this space.&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/SBHmDWygm6Q" height="1" width="1" alt=""/&gt;</content><summary>This post was originally published as "How blockchain can complement open source" on Opensource.com under CC BY-SA 4.0. If you prefer, you can also read the same post on Medium. Open Won Over ClosedThe Cathedral and The Bazaar is the classic open source story written 20 years ago by Eric Steven Raymond. In the story, Eric describes a new revolutionary software development model where complex softw...</summary><dc:creator>Bilgin Ibryam</dc:creator><dc:date>2018-10-01T11:43:00Z</dc:date><feedburner:origLink>http://www.ofbizian.com/2018/10/the-cathedral-and-bazaar-moving-from.html</feedburner:origLink></entry><entry><title>Hibernate OGM 5.4.0.CR1 release</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/wQfKkHzBivk/" /><category term="feed_group_name_hibernate" scheme="searchisko:content:tags" /><category term="feed_name_inrelationto" scheme="searchisko:content:tags" /><category term="hibernate ogm" scheme="searchisko:content:tags" /><category term="releases" scheme="searchisko:content:tags" /><author><name>Fabio Massimo Ercoli</name></author><id>searchisko:content:id:jbossorg_blog-hibernate_ogm_5_4_0_cr1_release</id><updated>2018-10-01T09:41:20Z</updated><published>2018-10-01T00:00:00Z</published><content type="html">&lt;div id="preamble"&gt; &lt;div class="sectionbody"&gt; &lt;div class="paragraph"&gt; &lt;p&gt;&lt;a href="http://hibernate.org/ogm/releases/5.4/#get-it"&gt;Hibernate OGM 5.4.0.CR1&lt;/a&gt; has been released!&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;Here’s a list of the main changes:&lt;/p&gt; &lt;/div&gt; &lt;div class="ulist"&gt; &lt;ul&gt; &lt;li&gt; &lt;p&gt;We support Infinispan remote transactions over HotRod client&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Java types &lt;code&gt;java.time.LocalDate&lt;/code&gt;, &lt;code&gt;java.time.LocalDateTime&lt;/code&gt; and &lt;code&gt;java.time.LocalTime&lt;/code&gt; are natively supported as field types&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;It is possible to configure the MongoDB &lt;code&gt;ReadConcern&lt;/code&gt; strategy.&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;More details available in the &lt;a href="https://hibernate.atlassian.net/secure/ReleaseNote.jspa?projectId=10160&amp;amp;version=31690"&gt;release notes&lt;/a&gt;.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="components-upgrade"&gt;&lt;a class="anchor" href="#components-upgrade"&gt;&lt;/a&gt;Components upgrade&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;div class="ulist"&gt; &lt;ul&gt; &lt;li&gt; &lt;p&gt;Hibernate ORM 5.3.4.Final&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Hibernate Search 5.10.4.Final&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Infinispan 9.3.3.Final&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="use-java-type-localdate"&gt;&lt;a class="anchor" href="#use-java-type-localdate"&gt;&lt;/a&gt;Use java.type.LocalDate&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;div class="paragraph"&gt; &lt;p&gt;Now you can define an entity having &lt;code&gt;java.time&lt;/code&gt; fields. Such as:&lt;/p&gt; &lt;/div&gt; &lt;div class="exampleblock"&gt; &lt;div class="content"&gt; &lt;div class="listingblock"&gt; &lt;div class="content"&gt; &lt;pre class="CodeRay highlight"&gt;&lt;code data-lang="java"&gt;&lt;span style="color:#080;font-weight:bold"&gt;import&lt;/span&gt; &lt;span style="color:#B44;font-weight:bold"&gt;java.time.LocalDate&lt;/span&gt;; &lt;span style="color:#080;font-weight:bold"&gt;import&lt;/span&gt; &lt;span style="color:#B44;font-weight:bold"&gt;java.time.LocalDateTime&lt;/span&gt;; &lt;span style="color:#080;font-weight:bold"&gt;import&lt;/span&gt; &lt;span style="color:#B44;font-weight:bold"&gt;java.time.LocalTime&lt;/span&gt;; &lt;span style="color:#007"&gt;@Entity&lt;/span&gt; &lt;span style="color:#088;font-weight:bold"&gt;public&lt;/span&gt; &lt;span style="color:#339;font-weight:bold"&gt;class&lt;/span&gt; &lt;span style="color:#B06;font-weight:bold"&gt;LocalDateEntity&lt;/span&gt; { &lt;span style="color:#007"&gt;@Id&lt;/span&gt; &lt;span style="color:#088;font-weight:bold"&gt;private&lt;/span&gt; &lt;span style="color:#0a8;font-weight:bold"&gt;Integer&lt;/span&gt; id; &lt;span style="color:#088;font-weight:bold"&gt;private&lt;/span&gt; LocalDate day; &lt;i class="conum" data-value="1"&gt;&lt;/i&gt;&lt;b&gt;(1)&lt;/b&gt; &lt;span style="color:#088;font-weight:bold"&gt;private&lt;/span&gt; LocalDateTime moment; &lt;i class="conum" data-value="2"&gt;&lt;/i&gt;&lt;b&gt;(2)&lt;/b&gt; &lt;span style="color:#088;font-weight:bold"&gt;private&lt;/span&gt; LocalTime time; &lt;i class="conum" data-value="3"&gt;&lt;/i&gt;&lt;b&gt;(3)&lt;/b&gt; }&lt;/code&gt;&lt;/pre&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;Hibernate OGM will handle the fields:&lt;/p&gt; &lt;/div&gt; &lt;div class="olist arabic"&gt; &lt;ol class="arabic"&gt; &lt;li&gt; &lt;p&gt;day, as a &lt;code&gt;java.time.LocalDate&lt;/code&gt; type&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;moment, as a &lt;code&gt;java.time.LocalDateTime&lt;/code&gt; type&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;time, as a &lt;code&gt;java.time.LocalTime&lt;/code&gt; type&lt;/p&gt; &lt;/li&gt; &lt;/ol&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="where-can-i-get-hibernate-ogm"&gt;&lt;a class="anchor" href="#where-can-i-get-hibernate-ogm"&gt;&lt;/a&gt;Where can I get Hibernate OGM?&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;div class="paragraph"&gt; &lt;p&gt;You can include the dialect of your choice in your project using the following Maven coordinates:&lt;/p&gt; &lt;/div&gt; &lt;div class="ulist"&gt; &lt;ul&gt; &lt;li&gt; &lt;p&gt;&lt;a href="http://infinispan.org"&gt;Infinispan&lt;/a&gt;&lt;/p&gt; &lt;div class="ulist"&gt; &lt;ul&gt; &lt;li&gt; &lt;p&gt;Remote: &lt;em&gt;org.hibernate.ogm:hibernate-ogm-infinispan-remote:5.4.0.CR1&lt;/em&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Embedded: &lt;em&gt;org.hibernate.ogm:hibernate-ogm-infinispan-embedded:5.4.0.CR1&lt;/em&gt;&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://www.mongodb.com"&gt;MongoDB&lt;/a&gt;: &lt;em&gt;org.hibernate.ogm:hibernate-ogm-mongodb:5.4.0.CR1&lt;/em&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="http://neo4j.com"&gt;Neo4j&lt;/a&gt;: &lt;em&gt;org.hibernate.ogm:hibernate-ogm-neo4j:5.4.0.CR1&lt;/em&gt;&lt;/p&gt; &lt;div class="ulist"&gt; &lt;ul&gt; &lt;li&gt; &lt;p&gt;Infinispan Remote: &lt;em&gt;org.hibernate.ogm:hibernate-ogm-featurepack-infinispan-remote:5.4.0.CR1&lt;/em&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Infinispan Embedded: &lt;em&gt;org.hibernate.ogm:hibernate-ogm-featurepack-infinispan-embedded:5.4.0.CR1&lt;/em&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;MongoDB: &lt;em&gt;org.hibernate.ogm:hibernate-ogm-featurepack-mongodb:5.4.0.CR1&lt;/em&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Neo4j: &lt;em&gt;org.hibernate.ogm:hibernate-ogm-featurepack-neo4j:5.4.0.CR1&lt;/em&gt;&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;Alternatively, you can download archives containing all the binaries, source code and documentation &lt;a href="https://sourceforge.net/projects/hibernate/files/hibernate-ogm/5.4.0.CR1"&gt;from Sourceforge&lt;/a&gt;.&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;If you are interested about available versions, you can check the official &lt;a href="http://hibernate.org/ogm/releases"&gt;Hibernate OGM download page&lt;/a&gt;.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="how-can-i-get-in-touch"&gt;&lt;a class="anchor" href="#how-can-i-get-in-touch"&gt;&lt;/a&gt;How can I get in touch?&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;div class="paragraph"&gt; &lt;p&gt;You can find us through the following channels:&lt;/p&gt; &lt;/div&gt; &lt;div class="ulist"&gt; &lt;ul&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://discourse.hibernate.org/c/hibernate-ogm"&gt;User forum&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://hibernate.atlassian.net/browse/OGM"&gt;Issue tracker&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="http://lists.jboss.org/pipermail/hibernate-dev/"&gt;Mailing list&lt;/a&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="http://stackoverflow.com"&gt;Stack Overflow&lt;/a&gt;: we monitor the tag &lt;em&gt;hibernate-ogm&lt;/em&gt;&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;a href="https://www.hipchat.com/gXEjW5Wgg"&gt;HipChat&lt;/a&gt;: Hibernate OGM hipchat room&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="contributions"&gt;&lt;a class="anchor" href="#contributions"&gt;&lt;/a&gt;Contributions&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;div class="paragraph"&gt; &lt;p&gt;Some of the new features have been contributed by Aleksandr Mylnikov. Thanks a lot Aleksandr!&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;Looking forward to hearing your feedback!&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/wQfKkHzBivk" height="1" width="1" alt=""/&gt;</content><summary>Hibernate OGM 5.4.0.CR1 has been released! Here’s a list of the main changes: We support Infinispan remote transactions over HotRod client Java types java.time.LocalDate, java.time.LocalDateTime and java.time.LocalTime are natively supported as field types It is possible to configure the MongoDB ReadConcern strategy. More details available in the release notes. Components upgrade Hibernate ORM 5.3...</summary><dc:creator>Fabio Massimo Ercoli</dc:creator><dc:date>2018-10-01T00:00:00Z</dc:date><feedburner:origLink>http://in.relation.to/2018/10/01/hibernate-ogm-5-4-CR1-released/</feedburner:origLink></entry><entry><title>Using Git for configuration history</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/MqpxTxyYw0A/" /><category term="cli" scheme="searchisko:content:tags" /><category term="configuration" scheme="searchisko:content:tags" /><category term="feed_group_name_jbossas" scheme="searchisko:content:tags" /><category term="feed_name_wildfly" scheme="searchisko:content:tags" /><category term="history" scheme="searchisko:content:tags" /><category term="wildfly" scheme="searchisko:content:tags" /><author><name>Emmanuel Hugonnet</name></author><id>searchisko:content:id:jbossorg_blog-using_git_for_configuration_history</id><updated>2018-09-28T18:00:00Z</updated><published>2018-09-28T18:00:00Z</published><content type="html">&lt;div id="preamble"&gt; &lt;div class="sectionbody"&gt; &lt;div class="paragraph"&gt; &lt;p&gt;Until now the history of configuration in WildFly was using the folder + filename pattern. Now we have moved to a proper SCM integrating Git to manage history.&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;You can now take advantage of a full Git support for your configuration history:&lt;/p&gt; &lt;/div&gt; &lt;div class="ulist"&gt; &lt;ul&gt; &lt;li&gt; &lt;p&gt;every change in your configuration is now a commit.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;you can use branches to develop in parallel.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;you can create tags for stable points in your configuration.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;pull configuration from a remote repository.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;push your configuration history to a remote repository.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;use the git-bisect tool at your disposal when things go wrong.&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;Now if we execute a management operation that modifies the model, for example adding a new system property using the CLI:&lt;/p&gt; &lt;/div&gt; &lt;div class="listingblock"&gt; &lt;div class="content"&gt; &lt;pre class="CodeRay highlight"&gt;&lt;code data-lang="ruby"&gt;[standalone&lt;span class="instance-variable"&gt;@localhost&lt;/span&gt;:&lt;span class="integer"&gt;9990&lt;/span&gt; /] /system-property=&lt;span class="key"&gt;test&lt;/span&gt;:add(value=&lt;span class="string"&gt;&lt;span class="delimiter"&gt;&amp;quot;&lt;/span&gt;&lt;span class="content"&gt;test123&lt;/span&gt;&lt;span class="delimiter"&gt;&amp;quot;&lt;/span&gt;&lt;/span&gt;) {&lt;span class="string"&gt;&lt;span class="delimiter"&gt;&amp;quot;&lt;/span&gt;&lt;span class="content"&gt;outcome&lt;/span&gt;&lt;span class="delimiter"&gt;&amp;quot;&lt;/span&gt;&lt;/span&gt; =&amp;gt; &lt;span class="string"&gt;&lt;span class="delimiter"&gt;&amp;quot;&lt;/span&gt;&lt;span class="content"&gt;success&lt;/span&gt;&lt;span class="delimiter"&gt;&amp;quot;&lt;/span&gt;&lt;/span&gt;}&lt;/code&gt;&lt;/pre&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;What happens is:&lt;/p&gt; &lt;/div&gt; &lt;div class="ulist"&gt; &lt;ul&gt; &lt;li&gt; &lt;p&gt;The change is applied to the configuration file.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;The configuration file is added to a new commit.&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt; &lt;/div&gt; &lt;div class="admonitionblock important"&gt; &lt;table&gt; &lt;tr&gt; &lt;td class="icon"&gt; &lt;i class="fa icon-important" title="Important"&gt;&lt;/i&gt; &lt;/td&gt; &lt;td class="content"&gt; &lt;div class="paragraph"&gt; &lt;p&gt;The notion of configuration has been updated with the Git support. It covers more than &amp;apos;just&amp;apos; the &lt;code&gt;standalone.xml&lt;/code&gt; history but also the content files (aka managed deployments).&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;Thus even your deployments are in history, which makes sense in a way since those deployments appear in the configuration file.&lt;/p&gt; &lt;/div&gt; &lt;/td&gt; &lt;/tr&gt; &lt;/table&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_starting_with_a_local_git_repository"&gt;Starting with a local Git repository&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;div class="paragraph"&gt; &lt;p&gt;To start using Git you don’t have to create the repository, WildFly can do that for you. Just start your server with the following command line:&lt;/p&gt; &lt;/div&gt; &lt;div class="listingblock"&gt; &lt;div class="content"&gt; &lt;pre class="CodeRay highlight"&gt;&lt;code data-lang="bash"&gt;$ __WILDFLY_HOME__/bin/standalone.sh --git-repo=local --git-branch=my_branch&lt;/code&gt;&lt;/pre&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;If a &lt;em&gt;--git-branch&lt;/em&gt; parameter is added then the repository will be checked out from the supplied branch. Please note that the branch will not be automatically created and must already exist in the repository. By default, if no parameter is specified, the branch &lt;code&gt;master&lt;/code&gt; will be used. If a &lt;em&gt;--git-branch&lt;/em&gt; parameter is added then the repository will be checked out from the supplied branch. Please note that the branch will not be automatically created and must already exist in the repository. By default, if no parameter is specified, the branch &lt;code&gt;master&lt;/code&gt; will be used.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_starting_with_a_remote_git_repository"&gt;Starting with a remote Git Repository&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;div class="paragraph"&gt; &lt;p&gt;To start WildFly with a configuration from a remote Git repository is simple too, just use the following command line:&lt;/p&gt; &lt;/div&gt; &lt;div class="listingblock"&gt; &lt;div class="content"&gt; &lt;pre class="CodeRay highlight"&gt;&lt;code data-lang="bash"&gt;$ __WILDFLY_HOME__/bin/standalone.sh --git-repo=https://github.com/USER_NAME/wildfly-config.git --git-branch=master&lt;/code&gt;&lt;/pre&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="admonitionblock important"&gt; &lt;table&gt; &lt;tr&gt; &lt;td class="icon"&gt; &lt;i class="fa icon-important" title="Important"&gt;&lt;/i&gt; &lt;/td&gt; &lt;td class="content"&gt; &lt;div class="paragraph"&gt; &lt;p&gt;Be careful with this as the first step is to delete the configuration files to avoid conflicts when pulling for the first time.&lt;/p&gt; &lt;/div&gt; &lt;/td&gt; &lt;/tr&gt; &lt;/table&gt; &lt;/div&gt; &lt;div class="admonitionblock note"&gt; &lt;table&gt; &lt;tr&gt; &lt;td class="icon"&gt; &lt;i class="fa icon-note" title="Note"&gt;&lt;/i&gt; &lt;/td&gt; &lt;td class="content"&gt; &lt;div class="paragraph"&gt; &lt;p&gt;Note that you can use remote aliases if you have added them to your &lt;code&gt;.gitconfig&lt;/code&gt;.&lt;/p&gt; &lt;/div&gt; &lt;/td&gt; &lt;/tr&gt; &lt;/table&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_snapshots"&gt;Snapshots&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;div class="paragraph"&gt; &lt;p&gt;In addition to the commits taken by the server as described above, you can manually take snapshots which will be stored as &lt;code&gt;tags&lt;/code&gt; in the Git repository.&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;The ability to take a snapshot has been enhanced to allow you to add a comment to it. This comment will be used when creating the Git tag.&lt;/p&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;This is how you can take a snapshot from the JBoss CLI tool:&lt;/p&gt; &lt;/div&gt; &lt;div class="listingblock"&gt; &lt;div class="content"&gt; &lt;pre class="CodeRay highlight"&gt;&lt;code data-lang="ruby"&gt;[standalone&lt;span class="instance-variable"&gt;@localhost&lt;/span&gt;:&lt;span class="integer"&gt;9990&lt;/span&gt; /] &lt;span class="symbol"&gt;:take&lt;/span&gt;-snapshot(name=&lt;span class="string"&gt;&lt;span class="delimiter"&gt;&amp;quot;&lt;/span&gt;&lt;span class="content"&gt;snapshot&lt;/span&gt;&lt;span class="delimiter"&gt;&amp;quot;&lt;/span&gt;&lt;/span&gt;, comment=&lt;span class="string"&gt;&lt;span class="delimiter"&gt;&amp;quot;&lt;/span&gt;&lt;span class="content"&gt;1st snapshot&lt;/span&gt;&lt;span class="delimiter"&gt;&amp;quot;&lt;/span&gt;&lt;/span&gt;) { &lt;span class="string"&gt;&lt;span class="delimiter"&gt;&amp;quot;&lt;/span&gt;&lt;span class="content"&gt;outcome&lt;/span&gt;&lt;span class="delimiter"&gt;&amp;quot;&lt;/span&gt;&lt;/span&gt; =&amp;gt; &lt;span class="string"&gt;&lt;span class="delimiter"&gt;&amp;quot;&lt;/span&gt;&lt;span class="content"&gt;success&lt;/span&gt;&lt;span class="delimiter"&gt;&amp;quot;&lt;/span&gt;&lt;/span&gt;, &lt;span class="string"&gt;&lt;span class="delimiter"&gt;&amp;quot;&lt;/span&gt;&lt;span class="content"&gt;result&lt;/span&gt;&lt;span class="delimiter"&gt;&amp;quot;&lt;/span&gt;&lt;/span&gt; =&amp;gt; &lt;span class="string"&gt;&lt;span class="delimiter"&gt;&amp;quot;&lt;/span&gt;&lt;span class="content"&gt;1st snapshot&lt;/span&gt;&lt;span class="delimiter"&gt;&amp;quot;&lt;/span&gt;&lt;/span&gt; }&lt;/code&gt;&lt;/pre&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;You can also use the CLI to list all the snapshots:&lt;/p&gt; &lt;/div&gt; &lt;div class="listingblock"&gt; &lt;div class="content"&gt; &lt;pre class="CodeRay highlight"&gt;&lt;code data-lang="ruby"&gt;[standalone&lt;span class="instance-variable"&gt;@localhost&lt;/span&gt;:&lt;span class="integer"&gt;9990&lt;/span&gt; /] &lt;span class="symbol"&gt;:list&lt;/span&gt;-snapshots { &lt;span class="string"&gt;&lt;span class="delimiter"&gt;&amp;quot;&lt;/span&gt;&lt;span class="content"&gt;outcome&lt;/span&gt;&lt;span class="delimiter"&gt;&amp;quot;&lt;/span&gt;&lt;/span&gt; =&amp;gt; &lt;span class="string"&gt;&lt;span class="delimiter"&gt;&amp;quot;&lt;/span&gt;&lt;span class="content"&gt;success&lt;/span&gt;&lt;span class="delimiter"&gt;&amp;quot;&lt;/span&gt;&lt;/span&gt;, &lt;span class="string"&gt;&lt;span class="delimiter"&gt;&amp;quot;&lt;/span&gt;&lt;span class="content"&gt;result&lt;/span&gt;&lt;span class="delimiter"&gt;&amp;quot;&lt;/span&gt;&lt;/span&gt; =&amp;gt; { &lt;span class="string"&gt;&lt;span class="delimiter"&gt;&amp;quot;&lt;/span&gt;&lt;span class="content"&gt;directory&lt;/span&gt;&lt;span class="delimiter"&gt;&amp;quot;&lt;/span&gt;&lt;/span&gt; =&amp;gt; &lt;span class="string"&gt;&lt;span class="delimiter"&gt;&amp;quot;&lt;/span&gt;&lt;span class="delimiter"&gt;&amp;quot;&lt;/span&gt;&lt;/span&gt;, &lt;span class="string"&gt;&lt;span class="delimiter"&gt;&amp;quot;&lt;/span&gt;&lt;span class="content"&gt;names&lt;/span&gt;&lt;span class="delimiter"&gt;&amp;quot;&lt;/span&gt;&lt;/span&gt; =&amp;gt; [ &lt;span class="string"&gt;&lt;span class="delimiter"&gt;&amp;quot;&lt;/span&gt;&lt;span class="content"&gt;snapshot : 1st snapshot&lt;/span&gt;&lt;span class="delimiter"&gt;&amp;quot;&lt;/span&gt;&lt;/span&gt;, &lt;span class="string"&gt;&lt;span class="delimiter"&gt;&amp;quot;&lt;/span&gt;&lt;span class="content"&gt;refs/tags/snapshot&lt;/span&gt;&lt;span class="delimiter"&gt;&amp;quot;&lt;/span&gt;&lt;/span&gt;, &lt;span class="string"&gt;&lt;span class="delimiter"&gt;&amp;quot;&lt;/span&gt;&lt;span class="content"&gt;snapshot2 : 2nd snapshot&lt;/span&gt;&lt;span class="delimiter"&gt;&amp;quot;&lt;/span&gt;&lt;/span&gt;, &lt;span class="string"&gt;&lt;span class="delimiter"&gt;&amp;quot;&lt;/span&gt;&lt;span class="content"&gt;refs/tags/snapshot2&lt;/span&gt;&lt;span class="delimiter"&gt;&amp;quot;&lt;/span&gt;&lt;/span&gt; ] } }&lt;/code&gt;&lt;/pre&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;To delete a particular snapshot:&lt;/p&gt; &lt;/div&gt; &lt;div class="listingblock"&gt; &lt;div class="content"&gt; &lt;pre class="CodeRay highlight"&gt;&lt;code data-lang="ruby"&gt;[standalone&lt;span class="instance-variable"&gt;@localhost&lt;/span&gt;:&lt;span class="integer"&gt;9990&lt;/span&gt; /] &lt;span class="symbol"&gt;:delete&lt;/span&gt;-snapshot(name=&lt;span class="string"&gt;&lt;span class="delimiter"&gt;&amp;quot;&lt;/span&gt;&lt;span class="content"&gt;snapshot2&lt;/span&gt;&lt;span class="delimiter"&gt;&amp;quot;&lt;/span&gt;&lt;/span&gt;) {&lt;span class="string"&gt;&lt;span class="delimiter"&gt;&amp;quot;&lt;/span&gt;&lt;span class="content"&gt;outcome&lt;/span&gt;&lt;span class="delimiter"&gt;&amp;quot;&lt;/span&gt;&lt;/span&gt; =&amp;gt; &lt;span class="string"&gt;&lt;span class="delimiter"&gt;&amp;quot;&lt;/span&gt;&lt;span class="content"&gt;success&lt;/span&gt;&lt;span class="delimiter"&gt;&amp;quot;&lt;/span&gt;&lt;/span&gt;}&lt;/code&gt;&lt;/pre&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="admonitionblock note"&gt; &lt;table&gt; &lt;tr&gt; &lt;td class="icon"&gt; &lt;i class="fa icon-note" title="Note"&gt;&lt;/i&gt; &lt;/td&gt; &lt;td class="content"&gt; &lt;div class="paragraph"&gt; &lt;p&gt;Note that this is a real Git repository, thus using the git client of your choice you can list those tags, or browse the history.&lt;/p&gt; &lt;/div&gt; &lt;/td&gt; &lt;/tr&gt; &lt;/table&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_publishing"&gt;Publishing&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;div class="paragraph"&gt; &lt;p&gt;You may &amp;apos;publish&amp;apos; your changes on a remote repository (provided you have write access to it) so you can share them. For example, if you want to publish on GitHub, you need to create a token and allow for &lt;em&gt;full control&lt;/em&gt; of the repository. Then use that token in an Elytron configuration file like this:&lt;/p&gt; &lt;/div&gt; &lt;div class="listingblock"&gt; &lt;div class="content"&gt; &lt;pre class="CodeRay highlight"&gt;&lt;code data-lang="xml"&gt;&lt;span class="preprocessor"&gt;&amp;lt;?xml version=&amp;quot;1.0&amp;quot; encoding=&amp;quot;UTF-8&amp;quot;?&amp;gt;&lt;/span&gt; &lt;span class="tag"&gt;&amp;lt;configuration&amp;gt;&lt;/span&gt; &lt;span class="tag"&gt;&amp;lt;authentication-client&lt;/span&gt; &lt;span class="attribute-name"&gt;xmlns&lt;/span&gt;=&lt;span class="string"&gt;&lt;span class="delimiter"&gt;&amp;quot;&lt;/span&gt;&lt;span class="content"&gt;urn:elytron:1.1&lt;/span&gt;&lt;span class="delimiter"&gt;&amp;quot;&lt;/span&gt;&lt;/span&gt;&lt;span class="tag"&gt;&amp;gt;&lt;/span&gt; &lt;span class="tag"&gt;&amp;lt;authentication-rules&amp;gt;&lt;/span&gt; &lt;span class="tag"&gt;&amp;lt;rule&lt;/span&gt; &lt;span class="attribute-name"&gt;use-configuration&lt;/span&gt;=&lt;span class="string"&gt;&lt;span class="delimiter"&gt;&amp;quot;&lt;/span&gt;&lt;span class="content"&gt;test-login&lt;/span&gt;&lt;span class="delimiter"&gt;&amp;quot;&lt;/span&gt;&lt;/span&gt;&lt;span class="tag"&gt;&amp;gt;&lt;/span&gt; &lt;span class="tag"&gt;&amp;lt;/rule&amp;gt;&lt;/span&gt; &lt;span class="tag"&gt;&amp;lt;/authentication-rules&amp;gt;&lt;/span&gt; &lt;span class="tag"&gt;&amp;lt;authentication-configurations&amp;gt;&lt;/span&gt; &lt;span class="tag"&gt;&amp;lt;configuration&lt;/span&gt; &lt;span class="attribute-name"&gt;name&lt;/span&gt;=&lt;span class="string"&gt;&lt;span class="delimiter"&gt;&amp;quot;&lt;/span&gt;&lt;span class="content"&gt;test-login&lt;/span&gt;&lt;span class="delimiter"&gt;&amp;quot;&lt;/span&gt;&lt;/span&gt;&lt;span class="tag"&gt;&amp;gt;&lt;/span&gt; &lt;span class="tag"&gt;&amp;lt;sasl-mechanism-selector&lt;/span&gt; &lt;span class="attribute-name"&gt;selector&lt;/span&gt;=&lt;span class="string"&gt;&lt;span class="delimiter"&gt;&amp;quot;&lt;/span&gt;&lt;span class="content"&gt;BASIC&lt;/span&gt;&lt;span class="delimiter"&gt;&amp;quot;&lt;/span&gt;&lt;/span&gt; &lt;span class="tag"&gt;/&amp;gt;&lt;/span&gt; &lt;span class="tag"&gt;&amp;lt;set-user-name&lt;/span&gt; &lt;span class="attribute-name"&gt;name&lt;/span&gt;=&lt;span class="string"&gt;&lt;span class="delimiter"&gt;&amp;quot;&lt;/span&gt;&lt;span class="content"&gt;$GITHUB_USERNAME&lt;/span&gt;&lt;span class="delimiter"&gt;&amp;quot;&lt;/span&gt;&lt;/span&gt; &lt;span class="tag"&gt;/&amp;gt;&lt;/span&gt; &lt;span class="tag"&gt;&amp;lt;credentials&amp;gt;&lt;/span&gt; &lt;span class="tag"&gt;&amp;lt;clear-password&lt;/span&gt; &lt;span class="attribute-name"&gt;password&lt;/span&gt;=&lt;span class="string"&gt;&lt;span class="delimiter"&gt;&amp;quot;&lt;/span&gt;&lt;span class="content"&gt;$GITHUB_TOKEN&lt;/span&gt;&lt;span class="delimiter"&gt;&amp;quot;&lt;/span&gt;&lt;/span&gt; &lt;span class="tag"&gt;/&amp;gt;&lt;/span&gt; &lt;span class="tag"&gt;&amp;lt;/credentials&amp;gt;&lt;/span&gt; &lt;span class="tag"&gt;&amp;lt;set-mechanism-realm&lt;/span&gt; &lt;span class="attribute-name"&gt;name&lt;/span&gt;=&lt;span class="string"&gt;&lt;span class="delimiter"&gt;&amp;quot;&lt;/span&gt;&lt;span class="content"&gt;testRealm&lt;/span&gt;&lt;span class="delimiter"&gt;&amp;quot;&lt;/span&gt;&lt;/span&gt; &lt;span class="tag"&gt;/&amp;gt;&lt;/span&gt; &lt;span class="tag"&gt;&amp;lt;/configuration&amp;gt;&lt;/span&gt; &lt;span class="tag"&gt;&amp;lt;/authentication-configurations&amp;gt;&lt;/span&gt; &lt;span class="tag"&gt;&amp;lt;/authentication-client&amp;gt;&lt;/span&gt; &lt;span class="tag"&gt;&amp;lt;/configuration&amp;gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="paragraph"&gt; &lt;p&gt;Then, to publish your changes:&lt;/p&gt; &lt;/div&gt; &lt;div class="listingblock"&gt; &lt;div class="content"&gt; &lt;pre class="CodeRay highlight"&gt;&lt;code data-lang="ruby"&gt;[standalone&lt;span class="instance-variable"&gt;@localhost&lt;/span&gt;:&lt;span class="integer"&gt;9990&lt;/span&gt; /] &lt;span class="symbol"&gt;:publish&lt;/span&gt;-configuration(location=&lt;span class="string"&gt;&lt;span class="delimiter"&gt;&amp;quot;&lt;/span&gt;&lt;span class="content"&gt;origin&lt;/span&gt;&lt;span class="delimiter"&gt;&amp;quot;&lt;/span&gt;&lt;/span&gt;) {&lt;span class="string"&gt;&lt;span class="delimiter"&gt;&amp;quot;&lt;/span&gt;&lt;span class="content"&gt;outcome&lt;/span&gt;&lt;span class="delimiter"&gt;&amp;quot;&lt;/span&gt;&lt;/span&gt; =&amp;gt; &lt;span class="string"&gt;&lt;span class="delimiter"&gt;&amp;quot;&lt;/span&gt;&lt;span class="content"&gt;success&lt;/span&gt;&lt;span class="delimiter"&gt;&amp;quot;&lt;/span&gt;&lt;/span&gt;}&lt;/code&gt;&lt;/pre&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;div class="sect1"&gt; &lt;h2 id="_references"&gt;References&lt;/h2&gt; &lt;div class="sectionbody"&gt; &lt;div class="paragraph"&gt; &lt;p&gt;For the official documentation regarding Git history : &lt;a href="http://docs.wildfly.org/14/Admin_Guide.html#Configuration_file_git_history"&gt;Official Documentation&lt;/a&gt;.&lt;/p&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/MqpxTxyYw0A" height="1" width="1" alt=""/&gt;</content><summary>Until now the history of configuration in WildFly was using the folder + filename pattern. Now we have moved to a proper SCM integrating Git to manage history. You can now take advantage of a full Git support for your configuration history: every change in your configuration is now a commit. you can use branches to develop in parallel. you can create tags for stable points in your configuration. p...</summary><dc:creator>Emmanuel Hugonnet</dc:creator><dc:date>2018-09-28T18:00:00Z</dc:date><feedburner:origLink>http://wildfly.org/news/2018/09/28/Git-History/</feedburner:origLink></entry><entry><title>Dynamic IP address management in Open Virtual Network (OVN): Part Two</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/acOTlmbiDGU/" /><category term="Community" /><category term="Red Hat Enterprise Linux" /><category term="cloud networking" /><category term="IP address management" /><category term="IPAM" /><category term="network function virtualization" /><category term="networking" /><category term="NFV" /><category term="open virtual network" /><category term="Open vSwitch" /><category term="OVN" /><category term="switches" /><category term="virtual networking" /><author><name>Mark Michelson</name></author><id>https://developers.redhat.com/blog/?p=517817</id><updated>2018-09-27T19:37:44Z</updated><published>2018-09-27T19:37:44Z</published><content type="html">&lt;p&gt;In &lt;a href="https://developers.redhat.com/blog/2018/09/03/ovn-dynamic-ip-address-management/"&gt;part one&lt;/a&gt; of this series, we explored the dynamic IP address management (IPAM) capabilities of Open Virtual Network. We covered the &lt;code&gt;subnet&lt;/code&gt;, &lt;code&gt;ipv6_prefix&lt;/code&gt;, and &lt;code&gt;exclude_ips&lt;/code&gt; options on logical switches. We then saw how these options get applied to logical switch ports whose addresses have been set to the special &amp;#8220;dynamic&amp;#8221; value.  OVN, a subproject of &lt;a href="https://developers.redhat.com/blog/tag/open-vswitch/"&gt;Open vSwitch&lt;/a&gt;, is used for virtual networking in a number of Red Hat products like Red Hat OpenStack Platform, Red Hat Virtualization, and &lt;a href="https://developers.redhat.com/products/openshift/overview/"&gt;Red Hat OpenShift Container Platform&lt;/a&gt; in a future release.&lt;/p&gt; &lt;p&gt;In this part, we&amp;#8217;re going to explore some of the oversights and downsides in the feature, how those have been corrected, and what&amp;#8217;s in store for OVN in future versions.&lt;/p&gt; &lt;p&gt;&lt;span id="more-517817"&gt;&lt;/span&gt;&lt;/p&gt; &lt;h2&gt;Subnet changes&lt;/h2&gt; &lt;p&gt;Let&amp;#8217;s start by creating a simple logical switch with a couple of logical switch ports that use dynamic addresses:&lt;/p&gt; &lt;pre&gt;ovn-nbctl ls-add sw ovn-nbctl set Logical_Switch sw other_config:subnet=192.168.1.0/24 ovn-nbctl lsp-add sw sw-p1 ovn-nbctl lsp-set-addresses sw-p1 "dynamic" ovn-nbctl lsp-add sw sw-p2 ovn-nbctl lsp-set-addresses sw-p2 "dynamic" &lt;/pre&gt; &lt;p&gt;This creates a logical switch &lt;code&gt;sw&lt;/code&gt; with ports &lt;code&gt;sw-p1&lt;/code&gt; and &lt;code&gt;sw-p2&lt;/code&gt;. Port &lt;code&gt;sw-p1&lt;/code&gt; is assigned address 192.168.1.2, and port &lt;code&gt;sw-p2&lt;/code&gt; is assigned 192.168.1.3.&lt;/p&gt; &lt;p&gt;But wait: we made a mistake! We actually meant to set the subnet of the switch to &lt;code&gt;192.168.0.0/24&lt;/code&gt;. Let&amp;#8217;s correct it.&lt;/p&gt; &lt;pre&gt;ovn-nbctl set Logical_Switch sw other_config:subnet=192.168.0.0/24 &lt;/pre&gt; &lt;p&gt;All right, let&amp;#8217;s see how that&amp;#8217;s affected the logical switch ports&amp;#8217; addresses. If you are running the &lt;a href="http://www.openvswitch.org/"&gt;Open vSwitch&lt;/a&gt; (OVS) 2.9 series or earlier, then you&amp;#8217;ll see the following:&lt;/p&gt; &lt;pre&gt;$ ovn-nbctl --columns=name,dynamic_addresses,addresses list logical_switch_port name : "sw-p2" dynamic_addresses : "0a:00:00:00:00:02 192.168.1.3" addresses : [dynamic] name : "sw-p1" dynamic_addresses : "0a:00:00:00:00:01 192.168.1.2" addresses : [dynamic] &lt;/pre&gt; &lt;p&gt;Huh? The dynamic addresses didn&amp;#8217;t update. Prior to OVS version 2.10, the dynamic addresses will not automatically update if the &lt;code&gt;subnet&lt;/code&gt;, &lt;code&gt;ipv6_prefix&lt;/code&gt;, or &lt;code&gt;exclude_ips&lt;/code&gt; is updated. If you want the dynamic addresses to update, you need to clear the &lt;code&gt;dynamic_addresses&lt;/code&gt; from the affected logical switch ports. The easiest way to clear the &lt;code&gt;dynamic_addresses&lt;/code&gt; on all switch ports on switch &lt;code&gt;sw&lt;/code&gt; is the following:&lt;/p&gt; &lt;pre&gt;for port in $(ovn-nbctl --bare --columns=port find logical_switch name=sw) ; do ovn-nbctl clear logical_switch_port $port dynamic_addresses ; done &lt;/pre&gt; &lt;p&gt;Now let&amp;#8217;s take another look at the logical switch ports:&lt;/p&gt; &lt;pre&gt;$ ovn-nbctl --columns=name,dynamic_addresses,addresses list logical_switch_port name : "sw-p2" dynamic_addresses : "0a:00:00:00:00:03 192.168.0.2" addresses : [dynamic] name : "sw-p1" dynamic_addresses : "0a:00:00:00:00:04 192.168.0.3" addresses : [dynamic] &lt;/pre&gt; &lt;p&gt;There; that&amp;#8217;s better. There are a couple of things to note here. First, the order in which IP addresses get assigned to the switch ports is not always predictable. The final octet of the IP addresses assigned to the switch ports was swapped from what it had previously been. Also, the MAC addresses have been updated on each switch port. When we cleared the &lt;code&gt;dynamic_addresses&lt;/code&gt;, the MAC address assignments on the switch port got lost. As a result, &lt;code&gt;ovn-northd&lt;/code&gt; assigns new MAC addresses to the ports. Unfortunately, if you are using dynamic MAC addresses, this is unavoidable.&lt;/p&gt; &lt;p&gt;The good news is that starting with OVS 2.10.0, this is no longer necessary. Updating &lt;code&gt;subnet&lt;/code&gt;, &lt;code&gt;ipv6_prefix&lt;/code&gt;, or &lt;code&gt;exclude_ips&lt;/code&gt; on a logical switch will automatically update the &lt;code&gt;dynamic_addresses&lt;/code&gt; on all logical switch ports. The even better news is that only the affected values are updated, so in this particular case, the MAC addresses on each switch port stay the same.&lt;/p&gt; &lt;h2&gt;Conflicting addresses&lt;/h2&gt; &lt;p&gt;Let&amp;#8217;s take our switch from the previous section and add a third switch port to it:&lt;/p&gt; &lt;pre&gt;ovn-nbctl lsp-add sw sw-p3 ovn-nbctl lsp-set-addresses sw-p3 "00:00:00:00:00:03 192.168.0.3" &lt;/pre&gt; &lt;p&gt;And let&amp;#8217;s have a look at our switch ports at this point:&lt;/p&gt; &lt;pre&gt;name : "sw-p3" dynamic_addresses : [] addresses : ["00:00:00:00:00:03 192.168.0.3"] name : "sw-p2" dynamic_addresses : "0a:00:00:00:00:03 192.168.0.2" addresses : [dynamic] name : "sw-p1" dynamic_addresses : "0a:00:00:00:00:04 192.168.0.3" addresses : [dynamic] &lt;/pre&gt; &lt;p&gt;Oops—our new switch port has an address that conflicts with one of our dynamic addresses. This will result in errors when packets are sent. There are a couple of ways to clear this up.&lt;/p&gt; &lt;p&gt;One way to fix this is by clearing the &lt;code&gt;dynamic_addresses&lt;/code&gt; of &lt;code&gt;sw-p2&lt;/code&gt;, and then &lt;code&gt;sw-p2&lt;/code&gt; will get a new dynamic address assigned to it. As mentioned in the previous section, this also means that &lt;code&gt;sw-p2&lt;/code&gt; will get assigned a new MAC address.&lt;/p&gt; &lt;p&gt;The other way is to use &lt;code&gt;ovn-nbctl lsp-set-addresses&lt;/code&gt; on &lt;code&gt;sw-p3&lt;/code&gt; so that it has an address that doesn&amp;#8217;t conflict.&lt;/p&gt; &lt;p&gt;Starting with OVS version 2.10.0, this conflict can no longer occur. Instead, &lt;code&gt;sw-p2&lt;/code&gt; will automatically have its IP address updated to the next available address in the subnet. The code makes the assumption that statically assigned addresses are always correct and that dynamic addresses are &amp;#8220;wrong&amp;#8221; and need to be updated in the case of a conflict.&lt;/p&gt; &lt;p&gt;Starting with OVS version 2.11.0, it will be more difficult to cause this type of conflict. Watch what happens when we try the following with the current master of OVS:&lt;/p&gt; &lt;pre&gt;$ ovn-nbctl lsp-set-addresses sw-p3 "00:00:00:00:00:03 192.168.0.3" ovn-nbctl: Error on switch sw: duplicate IPv4 address 192.168.0.3 &lt;/pre&gt; &lt;p&gt;The message above indicates that the conflict is detected by &lt;code&gt;ovn-nbctl&lt;/code&gt; and the conflicting address is not set on &lt;code&gt;sw-p3&lt;/code&gt;. It still is possible to set a conflicting address on &lt;code&gt;sw-p3&lt;/code&gt; by using the following command:&lt;/p&gt; &lt;pre&gt;# Don't do this! $ ovn-nbctl set Logical_Switch_Port sw-p3 "00:00:00:00:00:03 192.168.0.3" &lt;/pre&gt; &lt;p&gt;Doing this will still result in the conflicting address being set in the northbound database, and it will result in &lt;code&gt;sw-p2&lt;/code&gt; being assigned a new IP address.&lt;/p&gt; &lt;h2&gt;Other fixed problems&lt;/h2&gt; &lt;p&gt;In this final section, we&amp;#8217;ll examine some more minor things that are fixed in the 2.10 series of OVS. These are much less likely to happen than the issues explored in the previous two sections, and they&amp;#8217;re similar. Here&amp;#8217;s a brief summary:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Prior to 2.10, if the MAC address on a switch port changes from being statically assigned to dynamically assigned, the MAC address would not be updated. In 2.10+, the MAC address is dynamically assigned.&lt;/li&gt; &lt;li&gt;Prior to 2.10, if the IPv6 address is dynamically assigned and the MAC address on the port changes, then the IPv6 address is not updated. In 2.10+, when the MAC address is changed, the IPv6 address is recalculated too.&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;The future of IPAM in OVN&lt;/h2&gt; &lt;p&gt;IPAM offers a handy way to have IP addresses and MAC addresses automatically get assigned to your logical switch ports. In &lt;a href="https://developers.redhat.com/blog/2018/09/03/ovn-dynamic-ip-address-management/"&gt;part 1&lt;/a&gt;, we explored the basics of enabling IPAM in OVN, and in this part, we saw some downsides that have been fixed recently. But what is still to come? New developments are focused not so much on fixing issues as on adding features.&lt;/p&gt; &lt;p&gt;One improvement in the pipe is to allow for the pool of assignable MAC addresses to be configured. As we have seen in these posts, OVN will assign MAC addresses that start with &amp;#8220;0a.&amp;#8221; But what about deployments where you want OVN to assign MAC addresses but you want to pick the range of MAC addresses to be assigned? This is currently being developed. One idea is to provide a start and end address, allowing OVN to assign addresses from that range. Another idea is to allow for an Organizational Unique Identifier (OUI) to be configured and assign OVN addresses using this OUI as a prefix.&lt;/p&gt; &lt;p&gt;Another improvement is to provide consistent pairings of IPv4 addresses and MAC addresses. Currently, OVN assigns MAC and IPv4 addresses independently of each other. However, it would be more friendly on ARP tables to try to assign the same IPv4 address with the same MAC address each time.&lt;/p&gt; &lt;p&gt;Both of the above ideas are currently in development, with a target of being available in the 2.11 series of OVS. I&amp;#8217;m sure those of you reading these blog posts have ideas for further features that could be added. If you do, feel free to leave a comment on this post with your suggestion.&lt;/p&gt; &lt;p&gt;&lt;a class="a2a_button_facebook" href="https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F09%2F27%2Fdynamic-ip-address-management-in-open-virtual-network-ovn-part-two%2F&amp;#38;linkname=Dynamic%20IP%20address%20management%20in%20Open%20Virtual%20Network%20%28OVN%29%3A%20Part%20Two" title="Facebook" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_twitter" href="https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F09%2F27%2Fdynamic-ip-address-management-in-open-virtual-network-ovn-part-two%2F&amp;#38;linkname=Dynamic%20IP%20address%20management%20in%20Open%20Virtual%20Network%20%28OVN%29%3A%20Part%20Two" title="Twitter" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_google_plus" href="https://www.addtoany.com/add_to/google_plus?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F09%2F27%2Fdynamic-ip-address-management-in-open-virtual-network-ovn-part-two%2F&amp;#38;linkname=Dynamic%20IP%20address%20management%20in%20Open%20Virtual%20Network%20%28OVN%29%3A%20Part%20Two" title="Google+" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_linkedin" href="https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F09%2F27%2Fdynamic-ip-address-management-in-open-virtual-network-ovn-part-two%2F&amp;#38;linkname=Dynamic%20IP%20address%20management%20in%20Open%20Virtual%20Network%20%28OVN%29%3A%20Part%20Two" title="LinkedIn" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_email" href="https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F09%2F27%2Fdynamic-ip-address-management-in-open-virtual-network-ovn-part-two%2F&amp;#38;linkname=Dynamic%20IP%20address%20management%20in%20Open%20Virtual%20Network%20%28OVN%29%3A%20Part%20Two" title="Email" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_print" href="https://www.addtoany.com/add_to/print?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F09%2F27%2Fdynamic-ip-address-management-in-open-virtual-network-ovn-part-two%2F&amp;#38;linkname=Dynamic%20IP%20address%20management%20in%20Open%20Virtual%20Network%20%28OVN%29%3A%20Part%20Two" title="Print" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_reddit" href="https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F09%2F27%2Fdynamic-ip-address-management-in-open-virtual-network-ovn-part-two%2F&amp;#38;linkname=Dynamic%20IP%20address%20management%20in%20Open%20Virtual%20Network%20%28OVN%29%3A%20Part%20Two" title="Reddit" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_flipboard" href="https://www.addtoany.com/add_to/flipboard?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F09%2F27%2Fdynamic-ip-address-management-in-open-virtual-network-ovn-part-two%2F&amp;#38;linkname=Dynamic%20IP%20address%20management%20in%20Open%20Virtual%20Network%20%28OVN%29%3A%20Part%20Two" title="Flipboard" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_dd addtoany_share_save addtoany_share" href="https://www.addtoany.com/share#url=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F09%2F27%2Fdynamic-ip-address-management-in-open-virtual-network-ovn-part-two%2F&amp;#38;title=Dynamic%20IP%20address%20management%20in%20Open%20Virtual%20Network%20%28OVN%29%3A%20Part%20Two" data-a2a-url="https://developers.redhat.com/blog/2018/09/27/dynamic-ip-address-management-in-open-virtual-network-ovn-part-two/" data-a2a-title="Dynamic IP address management in Open Virtual Network (OVN): Part Two"&gt;&lt;img src="https://static.addtoany.com/buttons/favicon.png" alt="Share"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2018/09/27/dynamic-ip-address-management-in-open-virtual-network-ovn-part-two/"&gt;Dynamic IP address management in Open Virtual Network (OVN): Part Two&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;RHD Blog&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/acOTlmbiDGU" height="1" width="1" alt=""/&gt;</content><summary type="html">&lt;p&gt;In part one of this series, we explored the dynamic IP address management (IPAM) capabilities of Open Virtual Network. We covered the subnet, ipv6_prefix, and exclude_ips options on logical switches. We then saw how these options get applied to logical switch ports whose addresses have been set to the special &amp;#8220;dynamic&amp;#8221; value.  OVN, a subproject [&amp;#8230;]&lt;/p&gt; &lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2018/09/27/dynamic-ip-address-management-in-open-virtual-network-ovn-part-two/"&gt;Dynamic IP address management in Open Virtual Network (OVN): Part Two&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;RHD Blog&lt;/a&gt;.&lt;/p&gt;</summary><wfw:commentRss xmlns:wfw="http://wellformedweb.org/CommentAPI/">https://developers.redhat.com/blog/2018/09/27/dynamic-ip-address-management-in-open-virtual-network-ovn-part-two/feed/</wfw:commentRss><slash:comments xmlns:slash="http://purl.org/rss/1.0/modules/slash/">0</slash:comments><post-id xmlns="com-wordpress:feed-additions:1">517817</post-id><dc:creator>Mark Michelson</dc:creator><dc:date>2018-09-27T19:37:44Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2018/09/27/dynamic-ip-address-management-in-open-virtual-network-ovn-part-two/</feedburner:origLink></entry><entry><title>Keycloak 4.5.0.Final Released</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/WE3eMcyLgN4/keycloak-450final-released.html" /><category term="feed_group_name_keycloak" scheme="searchisko:content:tags" /><category term="feed_name_keycloak" scheme="searchisko:content:tags" /><author><name>Stian Thorgersen</name></author><id>searchisko:content:id:jbossorg_blog-keycloak_4_5_0_final_released</id><updated>2018-09-27T06:58:43Z</updated><published>2018-09-27T06:58:00Z</published><content type="html">&lt;p&gt;To download the release go to the &lt;a href="http://www.keycloak.org/downloads"&gt;Keycloak homepage&lt;/a&gt;. &lt;p&gt;For details on what is included in the release check out the &lt;a href="https://www.keycloak.org/docs/latest/release_notes/index.html"&gt;Release notes&lt;/a&gt; &lt;p&gt;The full list of resolved issues is available in &lt;a href="https://issues.jboss.org/issues/?jql=project%20%3D%20keycloak%20and%20fixVersion%20%3D%204.5.0.Final"&gt;JIRA&lt;/a&gt;. &lt;p&gt;Before you upgrade remember to backup your database and check the &lt;a href="http://www.keycloak.org/docs/latest/upgrading/index.html"&gt;upgrade guide&lt;/a&gt; for anything that may have changed.&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/WE3eMcyLgN4" height="1" width="1" alt=""/&gt;</content><summary>To download the release go to the Keycloak homepage. For details on what is included in the release check out the Release notes The full list of resolved issues is available in JIRA. Before you upgrade remember to backup your database and check the upgrade guide for anything that may have changed.</summary><dc:creator>Stian Thorgersen</dc:creator><dc:date>2018-09-27T06:58:00Z</dc:date><feedburner:origLink>http://blog.keycloak.org/2018/09/keycloak-450final-released.html</feedburner:origLink></entry><entry><title>Source versus binary S2I workflows with Red Hat OpenShift Application Runtimes</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/VNVL4r0yZBg/" /><category term="Container Development Kit" /><category term="Modern App Dev" /><category term="Red Hat OpenShift Application Runtimes" /><category term="Red Hat OpenShift Container Platform" /><category term="binaries" /><category term="binary workflow" /><category term="containers" /><category term="Fabric8 Maven Plug-in" /><category term="FMP" /><category term="Red Hat OpenShift" /><category term="S21" /><category term="source workflow" /><category term="source-to-image" /><author><name>Fernando Lozano</name></author><id>https://developers.redhat.com/blog/?p=519917</id><updated>2018-09-26T20:19:11Z</updated><published>2018-09-26T20:19:11Z</published><content type="html">&lt;p&gt;Red Hat OpenShift supports two workflows for building container images for applications: the &lt;em&gt;source&lt;/em&gt; and the &lt;em&gt;binary&lt;/em&gt; workflows. The binary workflow is the primary focus of the &lt;a href="https://developers.redhat.com/products/rhoar/overview/"&gt;Red Hat OpenShift Application Runtimes&lt;/a&gt; and &lt;a href="https://developers.redhat.com/products/fuse/overview/"&gt;Red Hat Fuse&lt;/a&gt; product documentation and training, while the source workflow is the focus of most of the &lt;a href="https://developers.redhat.com/products/openshift/overview/"&gt;Red Hat OpenShift Container Platform&lt;/a&gt; product documentation and training. All of the standard OpenShift Quick Application Templates are based on the source workflow.&lt;/p&gt; &lt;p&gt;A developer might ask, “Can I use both workflows on the same project?” or, “Is there a reason to prefer one workflow over the other?” As a member of the team that developed Red Hat certification training for OpenShift and Red Hat Fuse, I had these questions myself and I hope that this article helps you find your own answers to these questions.&lt;/p&gt; &lt;p&gt;&lt;span id="more-519917"&gt;&lt;/span&gt;&lt;/p&gt; &lt;h2&gt;Comparing the binary and source workflows&lt;/h2&gt; &lt;p&gt;Because both workflows are based on the &lt;i&gt;source-to-image (S2I)&lt;/i&gt; feature, it may sound strange having a &lt;em&gt;binary&lt;/em&gt; option. Actually, both workflows rely on S2I builds using the &lt;em&gt;Source&lt;/em&gt; strategy. The key difference is that the source workflow generates deployable artifacts of your application inside OpenShift, while the binary workflow generates these binary artifacts outside OpenShift. Both of them build the application container image inside OpenShift.&lt;/p&gt; &lt;p&gt;In a sense, the binary workflow provides an application binary as the source for an OpenShift S2I build that generates a container image.&lt;/p&gt; &lt;p&gt;Take a simple application, such as the Vert.x-based &amp;#8220;Hello, World&amp;#8221; available on GitHub at &lt;a href="https://github.com/flozanorht/vertx-hello.git"&gt;https://github.com/flozanorht/vertx-hello.git&lt;/a&gt;. It can be run locally, as a standalone Java application, and it can be deployed on OpenShift using both workflows from the same sources.&lt;/p&gt; &lt;h3&gt;Using the binary workflow&lt;/h3&gt; &lt;p&gt;When using the binary workflow, developers would:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Clone the project to a local folder, so they can change the code to their liking and maybe test it outside of OpenShift.&lt;/li&gt; &lt;li&gt;Log in to OpenShift and create a new project.&lt;/li&gt; &lt;li&gt;Use the &lt;code&gt;mvn&lt;/code&gt; command, which in turn uses the &lt;em&gt;Fabric8 Maven Plug-in (FMP)&lt;/em&gt; to build the container image and create the OpenShift resources that describe the application. Maven performs the following tasks: &lt;ol type="a"&gt; &lt;li&gt;Generates the application package (an executable JAR)&lt;/li&gt; &lt;li&gt;Starts a binary build that creates the application container image, and streams the application package to the build pod&lt;/li&gt; &lt;li&gt;Creates OpenShift resources to deploy the application&lt;/li&gt; &lt;/ol&gt; &lt;/li&gt; &lt;li&gt;Use either curl or a web browser to test the application.&lt;/li&gt; &lt;/ol&gt; &lt;h3&gt;Using the source workflow&lt;/h3&gt; &lt;p&gt;When using the source workflow, developers would:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Clone the project to a local folder, so they can change the code to their liking and maybe test it outside of OpenShift.&lt;/li&gt; &lt;li&gt;Commit and push any changes to the origin git repository.&lt;/li&gt; &lt;li&gt;Log in to OpenShift and create a new project.&lt;/li&gt; &lt;li&gt;Use the &lt;code&gt;oc new-app&lt;/code&gt; command to build the container image and create the OpenShift resources that describe the application. &lt;ol type="a"&gt; &lt;li&gt;The OpenShift client command creates OpenShift resources to build and deploy the application.&lt;/li&gt; &lt;li&gt;The build configuration resource starts a source build that runs Maven to generate the application package (JAR) and create the application container image containing the application package.&lt;/li&gt; &lt;/ol&gt; &lt;/li&gt; &lt;li&gt;Expose the application service to the outside world.&lt;/li&gt; &lt;li&gt;Use either curl or a web browser to test the application.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;From an operational perspective, the main difference between both workflows lies between using the &lt;code&gt;mvn&lt;/code&gt; command or the &lt;code&gt;oc new-app&lt;/code&gt; command:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;The binary workflow relies on Maven to perform the heavy lifting, which most Java developers appreciate.&lt;/li&gt; &lt;li&gt;The source workflow, on the other hand, relies on the OpenShift client (the &lt;code&gt;oc&lt;/code&gt; command).&lt;/li&gt; &lt;/ul&gt; &lt;h3&gt;When to use each workflow&lt;/h3&gt; &lt;p&gt;Before declaring that you prefer the binary workflow because you already know Maven, consider that developers use the OpenShift client to monitor and troubleshoot their applications, so maybe performing a few tasks using Maven is not that big a win.&lt;/p&gt; &lt;p&gt;The source workflow requires that all changes are pushed to a network-accessible git repository, while the binary workflow works with your local changes. This difference comes from the fact that the source workflow builds your Java package (JAR) inside OpenShift, while the binary workflow takes the Java package you built locally without touching your source code.&lt;/p&gt; &lt;p&gt;For some developers, using the binary workflow saves time, because they perform local builds anyway to run unit tests and perform other tasks. For other developers, the source workflow brings the advantage that all heavy work is done by OpenShift. This means that the developer’s workstation does not need to have Maven, a Java compiler, and everything else installed. Instead, developers could use a low-powered PC, or even a tablet, to write their code, commit, and initiate an OpenShift build to test their applications.&lt;/p&gt; &lt;p&gt;Nothing prevents you from using both workflows to meet different goals. For example, developers can use the binary workflow to test their changes in a local minishift instance, including running unit tests, while a QA environment uses the source workflow to build the application container image, triggered by a webhook, and performs a set of integration tests in a dedicated, multi-node OpenShift cluster.&lt;/p&gt; &lt;h2&gt;Deploying an application using the binary and source workflows&lt;/h2&gt; &lt;p&gt;Let’s experiment using both workflows with the &lt;a href="https://github.com/flozanorht/vertx-hello.git"&gt;Vert.x-based &amp;#8220;Hello, World&amp;#8221; application&lt;/a&gt;. Don&amp;#8217;t worry if you are unfamiliar with Vert.x, because the application is very simple and ready to run. Focus on the Maven POM settings that allow the same project to work with both the binary and the source workflows.&lt;/p&gt; &lt;p&gt;Unlike most examples you&amp;#8217;ve probably seen elsewhere, my Vert.x application is configured to use supported dependencies from Red Hat OpenShift Application Runtimes instead of the upstream community artifacts. Do you need to be a Red Hat paying customer to follow the instructions? Not at all: you just need to register at the &lt;a href="http://developers.redhat.com/"&gt;Red Hat Developers website&lt;/a&gt;, and you&amp;#8217;ll get a free developer&amp;#8217;s subscription to Red Hat Enterprise Linux, Red Hat OpenShift Container Platform, Red Hat OpenShift Application Runtimes, and all of Red Hat&amp;#8217;s middleware and DevOps portfolio.&lt;/p&gt; &lt;p&gt;Fire up your minishift instance, and let&amp;#8217;s try both the source and binary builds on OpenShift. If you do not have a minishift instance, register with Red Hat Developers, &lt;a href="https://developers.redhat.com/products/cdk/download/"&gt;download and install the Red Hat Container Development Kit (CDK)&lt;/a&gt;, and follow the &lt;a href="https://developers.redhat.com/products/cdk/hello-world/"&gt;instructions to set up minishift&lt;/a&gt;, which provides a VM to run OpenShift in your local machine. Or, if you have access to a real OpenShift cluster, log in to it and follow the instructions in the next sections.&lt;/p&gt; &lt;h2&gt;Vert.x &amp;#8220;Hello, World&amp;#8221; using the binary workflow&lt;/h2&gt; &lt;p&gt;First, clone the sample Vert.x application. The following commands assume you are using a Linux machine, but it should not be hard to adapt them to a Windows or Mac machine. You can run the CDK in either of them.&lt;/p&gt; &lt;pre&gt;$ git clone https://github.com/flozanorht/vertx-hello.git $ cd vertx-hello&lt;/pre&gt; &lt;p&gt;Because we are using supported Maven artifacts, you need to configure your Maven installation to use the Red Hat Maven Repository. The &lt;code&gt;conf&lt;/code&gt; folder contains a sample Maven &lt;code&gt;settings.xml&lt;/code&gt; file that you can copy to your &lt;code&gt;~/.m2&lt;/code&gt; folder or use as an example of the changes to make.&lt;/p&gt; &lt;p&gt;Log in to OpenShift and create a test project. The following instructions assume you are using minishift and that your minishift instance is already running, but it should not be hard to adapt them to an external OpenShift cluster.&lt;/p&gt; &lt;pre&gt;$ oc login -u developer -p developer $ oc new-project binary&lt;/pre&gt; &lt;p&gt;Now comes the fun part: let the Fabric8 Maven Plug-in (FMP) do all the work.&lt;/p&gt; &lt;pre&gt;$ mvn -Popenshift fabric8:deploy … [INFO] Building Vert.x Hello, World 1.0 … [INFO] --- fabric8-maven-plugin:3.5.38:resource (fmp) @ vertx-hello --- [INFO] F8: Running in OpenShift mode [INFO] F8: Using docker image name of namespace: binary [INFO] F8: Running generator vertx [INFO] F8: vertx: Using ImageStreamTag 'redhat-openjdk18-openshift:1.3' as builder image … [INFO] --- maven-surefire-plugin:2.12.4:test (default-test) @ vertx-hello --- … Tests run: 3, Failures: 0, Errors: 0, Skipped: 0 … [INFO] --- fabric8-maven-plugin:3.5.38:build (fmp) @ vertx-hello --- [INFO] F8: Using OpenShift build with strategy S2I … [INFO] F8: Starting S2I Java Build ..... [INFO] F8: S2I binary build from fabric8-maven-plugin detected [INFO] F8: Copying binaries from /tmp/src/maven to /deployments ... [INFO] F8: ... done [INFO] F8: Pushing image 172.30.1.1:5000/binary/vertx-hello:1.0 … … [INFO] F8: Pushed 6/6 layers, 100% complete [INFO] F8: Push successful [INFO] F8: Build vertx-hello-s2i-1 Complete … [INFO] --- fabric8-maven-plugin:3.5.38:deploy (default-cli) @ vertx-hello --- … [INFO] BUILD SUCCESS …&lt;/pre&gt; &lt;p&gt;The configurations for the FMP are inside the Maven profile named &lt;code&gt;openshift&lt;/code&gt;. This profile allows you to build and run the application locally, without OpenShift. If you want to, invoke the Maven &lt;code&gt;package&lt;/code&gt; goal and run the JAR package from the target folder using &lt;code&gt;java -jar&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;The build takes some time; most of it is for downloading Maven artifacts. Generating and pushing the container image to the internal registry takes from a few seconds to a few minutes.&lt;/p&gt; &lt;p&gt;The FMP may lose the connection to the builder pod and display warning messages such as the following, which you can just ignore:&lt;/p&gt; &lt;pre&gt;[INFO] Current reconnect backoff is 1000 milliseconds (T0)&lt;/pre&gt; &lt;p&gt;The following error message can also be ignored:&lt;/p&gt; &lt;pre&gt;[ERROR] Exception in reconnect java.util.concurrent.RejectedExecutionException: Task java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask@741d5132 rejected from …&lt;/pre&gt; &lt;p&gt;The FMP already fixed these issues and the fixes should land in Red Hat OpenShift Application Runtimes in the near future.&lt;/p&gt; &lt;p&gt;Despite these messages, your build is successful. The FMP also created a route to allow access to your application. Find the host name assigned to your route:&lt;/p&gt; &lt;pre&gt;$ oc get route NAME          HOST/PORT                                  PATH     SERVICES      PORT      TERMINATION   WILDCARD vertx-hello   vertx-hello-binary.192.168.42.180.nip.io            vertx-hello   8080          None&lt;/pre&gt; &lt;p&gt;And then test your application using curl:&lt;/p&gt; &lt;pre&gt;$ curl http://vertx-hello-binary.192.168.42.180.nip.io/api/hello/Binary Hello Binary, from vertx-hello-binary.192.168.42.180.nip.io.&lt;/pre&gt; &lt;h3&gt;OpenShift resources with the binary workflow&lt;/h3&gt; &lt;p&gt;Note that the FMP also creates a few OpenShift resources: a service, a deployment configuration, and application pods:&lt;/p&gt; &lt;pre&gt;$ oc status In project binary on server https://192.168.42.180:8443 http://vertx-hello-binary.192.168.42.180.nip.io to pod port 8080 (svc/vertx-hello)  dc/vertx-hello deploys istag/vertx-hello:1.0 &amp;#60;-     bc/vertx-hello-s2i source builds uploaded code on openshift/redhat-openjdk18-openshift:1.3     deployment #1 deployed 8 minutes ago - 1 pod …&lt;/pre&gt; &lt;p&gt;The FMP creates OpenShift resources internal defaults. You can provide resource fragment files stored in the &lt;code&gt;src/main/fabric8 project&lt;/code&gt; folder to override these defaults. If you want to customize a readiness probe or resource limits for your pods, you have to edit these files.&lt;/p&gt; &lt;p&gt;You can peek inside the OpenShift build configuration that the FMP created for you. Note that the strategy is &lt;code&gt;Source&lt;/code&gt; and there is a binary input.&lt;/p&gt; &lt;pre&gt;$ oc get bc NAME              TYPE      FROM      LATEST vertx-hello-s2i   Source    Binary    1 $ oc describe bc vertx-hello-s2i … Strategy:    Source From Image:    ImageStreamTag openshift/redhat-openjdk18-openshift:1.3 Output to:    ImageStreamTag vertx-hello:1.0 Binary:       provided on build …&lt;/pre&gt; &lt;h3&gt;Rebuilds with the binary workflow&lt;/h3&gt; &lt;p&gt;The fact that the build requires a binary input means that you cannot simply start a new build using the  &lt;code&gt;oc start-build&lt;/code&gt; command. You also cannot use OpenShift webhooks to start a new build. If you need to perform a new build of the application, use the &lt;code&gt;mvn&lt;/code&gt; command again:&lt;/p&gt; &lt;pre&gt;$ mvn -Popenshift fabric8:deploy … [INFO] Building Vert.x Hello, World 1.0 … Tests run: 3, Failures: 0, Errors: 0, Skipped: 0 … [INFO] --- fabric8-maven-plugin:3.5.38:build (fmp) @ vertx-hello --- [INFO] F8: Using OpenShift build with strategy S2I … [INFO] F8: Pushed 6/6 layers, 100% complete [INFO] F8: Push successful [INFO] F8: Build vertx-hello-s2i-2 Complete … [INFO] BUILD SUCCESS …&lt;/pre&gt; &lt;p&gt;In the end, you get a new container image and a new application pod. Though the build logs seem to imply that the FMP re-creates (or updates) the OpenShift resources, it actually leaves them unchanged. If you need the FMP to update the OpenShift resources, you need to invoke the &lt;code&gt;fabric8:undeploy&lt;/code&gt; Maven goal and then invoke &lt;code&gt;fabric8:deploy&lt;/code&gt; again.&lt;/p&gt; &lt;h1&gt;Vert.x &amp;#8220;Hello, World&amp;#8221; using the source workflow&lt;/h1&gt; &lt;p&gt;First of all, clone the sample Vert.x application. The following commands assume you are using a Linux machine, but it should not be hard to adapt them to a Windows or Mac machine. You can run the CDK in either of them. If you have already cloned from the previous instructions, you can reuse the same cloned git repository and skip the next commands.&lt;/p&gt; &lt;pre&gt;$ git clone https://github.com/flozanorht/vertx-hello.git $ cd vertx-hello&lt;/pre&gt; &lt;p&gt;There is no need to configure Maven to use the Red Hat Maven Repository. The OpenShift builder images are already preconfigured with them. You will not perform local Maven builds anymore.&lt;/p&gt; &lt;p&gt;Log in to OpenShift and create a test project. The following instructions assume you are using minishift, but it should not be hard to adapt them to an external OpenShift cluster.&lt;/p&gt; &lt;pre&gt;$ oc login -u developer -p developer $ oc new-project source&lt;/pre&gt; &lt;p&gt;Now comes the fun part: let OpenShift&amp;#8217;s S2I feature do all the work.&lt;/p&gt; &lt;pre&gt;$ oc new-app redhat-openjdk18-openshift:1.3~https://github.com/flozanorht/vertx-hello.git … --&amp;#62; Creating resources ...     imagestream "vertx-hello" created     buildconfig "vertx-hello" created     deploymentconfig "vertx-hello" created     service "vertx-hello" created --&amp;#62; Success     Build scheduled, use 'oc logs -f bc/vertx-hello' to track its progress. …&lt;/pre&gt; &lt;p&gt;As suggested by the &lt;code&gt;oc new-app&lt;/code&gt; command, follow the OpenShift build logs, which includes the Maven build logs:&lt;/p&gt; &lt;pre&gt;$ oc logs -f bc/vertx-hello -f Cloning "https://github.com/flozanorht/vertx-hello.git" ... … Starting S2I Java Build ..... Maven build detected Initialising default settings /tmp/artifacts/configuration/settings.xml Setting MAVEN_OPTS to -XX:+UnlockExperimentalVMOptions -XX:+UseCGroupMemoryLimitForHeap -XX:+UseParallelOldGC -XX:MinHeapFreeRatio=10 -XX:MaxHeapFreeRatio=20 -XX:GCTimeRatio=4 -XX:AdaptiveSizePolicyWeight=90 -XX:MaxMetaspaceSize=100m -XX:+ExitOnOutOfMemoryError Found pom.xml ... … [INFO] Building Vert.x Hello, World 1.0 … [INFO] BUILD SUCCESS … Copying Maven artifacts from /tmp/src/target to /deployments ... … Pushed 6/6 layers, 100% complete Push successful&lt;/pre&gt; &lt;p&gt;Note that the source build invokes Maven with arguments that prevent it from running the FMP. Also, note that the end result of the build is a container image. The build pushes that container image into the internal registry.&lt;/p&gt; &lt;p&gt;The build takes some time; most of it is for downloading Maven artifacts. Generating and pushing the container image takes from a few seconds to a few minutes.&lt;/p&gt; &lt;p&gt;After the build finishes, you need to create a route to allow access to your application.&lt;/p&gt; &lt;pre&gt;$ oc expose svc vertx-hello route "vertx-hello" exposed&lt;/pre&gt; &lt;p&gt;Now find the host name assigned to your route:&lt;/p&gt; &lt;pre&gt;$ oc get route NAME          HOST/PORT                                  PATH      SERVICES      PORT       TERMINATION   WILDCARD vertx-hello   vertx-hello-source.192.168.42.180.nip.io             vertx-hello   8080-tcp           None&lt;/pre&gt; &lt;p&gt;And then test your application using &lt;code&gt;curl&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt;$ curl http://vertx-hello-source.192.168.42.180.nip.io/api/hello/Source Hello Source, from vertx-hello-source.192.168.42.180.nip.io.&lt;/pre&gt; &lt;h3&gt;OpenShift resources with the source workflow&lt;/h3&gt; &lt;p&gt;Note that the &lt;code&gt;oc new-app&lt;/code&gt; command also creates a few OpenShift resources: a service, a deployment configuration, and application pods:&lt;/p&gt; &lt;pre&gt;$ oc status In project source on server https://192.168.42.180:8443 http://vertx-hello-source.192.168.42.180.nip.io to pod port 8080-tcp (svc/vertx-hello)  dc/vertx-hello deploys istag/vertx-hello:latest &amp;#60;-     bc/vertx-hello source builds https://github.com/flozanorht/vertx-hello.git on openshift/redhat-openjdk18-openshift:1.3     deployment #1 deployed 3 minutes ago - 1 pod …&lt;/pre&gt; &lt;p&gt;The &lt;code&gt;oc new-app&lt;/code&gt; command creates OpenShift resources using hard-coded defaults. If you want to customize them, for example, to specify a readiness probe or resource limits for your pods, you have two options:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Find (or create) a suitable OpenShift template, and use this template as input for the &lt;code&gt;oc new-app&lt;/code&gt; command. The templates in the &lt;code&gt;openshift&lt;/code&gt; namespace are a good starting point.&lt;/li&gt; &lt;li&gt;Use OpenShift client commands, such as &lt;code&gt;oc set&lt;/code&gt; and &lt;code&gt;oc edit&lt;/code&gt; to change the resources in place.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;You can peek inside the OpenShift build configuration that the &lt;code&gt;oc new-app&lt;/code&gt; command created for you. Note that the strategy is &lt;code&gt;Source&lt;/code&gt; and there is a URL input.&lt;/p&gt; &lt;pre&gt;$ oc get bc NAME          TYPE      FROM      LATEST vertx-hello   Source    Git       1 $ oc describe bc vertx-hello … Strategy:    Source URL:       https://github.com/flozanorht/vertx-hello.git From Image:    ImageStreamTag openshift/redhat-openjdk18-openshift:1.3 Output to:    ImageStreamTag vertx-hello:latest …&lt;/pre&gt; &lt;h3&gt;Rebuilds with the source workflow&lt;/h3&gt; &lt;p&gt;If you need to perform a new build of the application, use the &lt;code&gt;oc start-build&lt;/code&gt; command:&lt;/p&gt; &lt;pre&gt;$ oc start-build vertx-hello build "vertx-hello-2" started&lt;/pre&gt; &lt;p&gt;You can use the same &lt;code&gt;oc logs&lt;/code&gt; command to watch the OpenShift build logs. In the end, you have a new container image and a new application pod ready and running. You do not need to re-create the route and other OpenShift resources.&lt;/p&gt; &lt;p&gt;Note that Maven downloads all dependencies again because the build pod only uses container ephemeral storage. There is no reuse of the Maven cache between S2I builds by default. Developers performing local Maven builds rely on their local Maven cache to speed up rebuilds. OpenShift provides an incremental builds feature that allows reusing the Maven cache between S2I builds. Incremental builds are a theme of a future post.&lt;/p&gt; &lt;p&gt;If you plan to use source builds, I recommend that you configure a Maven repository server such as Nexus. The &lt;code&gt;MAVEN_MIRROR_URL&lt;/code&gt; build environment variable points to the Maven repository server. This recommendation applies to any organization developing Java applications, but OpenShift makes this need even more pressing.&lt;/p&gt; &lt;h1&gt;Conclusion&lt;/h1&gt; &lt;p&gt;Understanding the OpenShift binary and source workflows allows a developer to make informed decisions about when to use which one. A Continuous Integration/Continuous Delivery (CI/CD) pipeline, managed by Jenkins or any other tool, could use both of them.&lt;/p&gt; &lt;p&gt;The binary and the source workflows make use of the same OpenShift S2I builder images. Both build container images inside OpenShift and push them to the internal registry. For most practical purposes, the container images generated by either the binary or source workflows are equivalent.&lt;/p&gt; &lt;p&gt;The binary workflow may better fit current developer workflows, especially when there is heavy customization of the project&amp;#8217;s POM. The source workflow allows for cloud-based development, such as &lt;a href="https://developers.redhat.com/products/openshiftio/overview/"&gt;Red Hat OpenShift.io&lt;/a&gt; and relieves the developer from needing a high-end workstation.&lt;/p&gt; &lt;p&gt;Red Hat Training provides two developer-oriented courses with content about OpenShift builds:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Red Hat OpenShift Development I: Containerizing Applications (&lt;a href="https://www.redhat.com/en/services/training/do288-red-hat-openshift-development-i-containerizing-applications"&gt;DO288&lt;/a&gt;)&lt;/li&gt; &lt;li&gt;Red Hat OpenShift Development II: Creating Microservices with Red Hat OpenShift Application Runtimes (&lt;a href="https://www.redhat.com/en/services/training/red-hat-openshift-development-ii"&gt;DO292&lt;/a&gt;)&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;You can also get the book &lt;a href="https://www.openshift.com/deploying-to-openshift/"&gt;Deploying to OpenShift&lt;/a&gt; by Graham Dumpleton for free.&lt;/p&gt; &lt;p&gt;&lt;a class="a2a_button_facebook" href="https://www.addtoany.com/add_to/facebook?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F09%2F26%2Fsource-versus-binary-s2i-workflows-with-red-hat-openshift-application-runtimes%2F&amp;#38;linkname=Source%20versus%20binary%20S2I%20workflows%20with%20Red%20Hat%20OpenShift%20Application%20Runtimes" title="Facebook" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_twitter" href="https://www.addtoany.com/add_to/twitter?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F09%2F26%2Fsource-versus-binary-s2i-workflows-with-red-hat-openshift-application-runtimes%2F&amp;#38;linkname=Source%20versus%20binary%20S2I%20workflows%20with%20Red%20Hat%20OpenShift%20Application%20Runtimes" title="Twitter" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_google_plus" href="https://www.addtoany.com/add_to/google_plus?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F09%2F26%2Fsource-versus-binary-s2i-workflows-with-red-hat-openshift-application-runtimes%2F&amp;#38;linkname=Source%20versus%20binary%20S2I%20workflows%20with%20Red%20Hat%20OpenShift%20Application%20Runtimes" title="Google+" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_linkedin" href="https://www.addtoany.com/add_to/linkedin?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F09%2F26%2Fsource-versus-binary-s2i-workflows-with-red-hat-openshift-application-runtimes%2F&amp;#38;linkname=Source%20versus%20binary%20S2I%20workflows%20with%20Red%20Hat%20OpenShift%20Application%20Runtimes" title="LinkedIn" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_email" href="https://www.addtoany.com/add_to/email?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F09%2F26%2Fsource-versus-binary-s2i-workflows-with-red-hat-openshift-application-runtimes%2F&amp;#38;linkname=Source%20versus%20binary%20S2I%20workflows%20with%20Red%20Hat%20OpenShift%20Application%20Runtimes" title="Email" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_print" href="https://www.addtoany.com/add_to/print?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F09%2F26%2Fsource-versus-binary-s2i-workflows-with-red-hat-openshift-application-runtimes%2F&amp;#38;linkname=Source%20versus%20binary%20S2I%20workflows%20with%20Red%20Hat%20OpenShift%20Application%20Runtimes" title="Print" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_reddit" href="https://www.addtoany.com/add_to/reddit?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F09%2F26%2Fsource-versus-binary-s2i-workflows-with-red-hat-openshift-application-runtimes%2F&amp;#38;linkname=Source%20versus%20binary%20S2I%20workflows%20with%20Red%20Hat%20OpenShift%20Application%20Runtimes" title="Reddit" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_button_flipboard" href="https://www.addtoany.com/add_to/flipboard?linkurl=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F09%2F26%2Fsource-versus-binary-s2i-workflows-with-red-hat-openshift-application-runtimes%2F&amp;#38;linkname=Source%20versus%20binary%20S2I%20workflows%20with%20Red%20Hat%20OpenShift%20Application%20Runtimes" title="Flipboard" rel="nofollow noopener" target="_blank"&gt;&lt;/a&gt;&lt;a class="a2a_dd addtoany_share_save addtoany_share" href="https://www.addtoany.com/share#url=https%3A%2F%2Fdevelopers.redhat.com%2Fblog%2F2018%2F09%2F26%2Fsource-versus-binary-s2i-workflows-with-red-hat-openshift-application-runtimes%2F&amp;#38;title=Source%20versus%20binary%20S2I%20workflows%20with%20Red%20Hat%20OpenShift%20Application%20Runtimes" data-a2a-url="https://developers.redhat.com/blog/2018/09/26/source-versus-binary-s2i-workflows-with-red-hat-openshift-application-runtimes/" data-a2a-title="Source versus binary S2I workflows with Red Hat OpenShift Application Runtimes"&gt;&lt;img src="https://static.addtoany.com/buttons/favicon.png" alt="Share"&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2018/09/26/source-versus-binary-s2i-workflows-with-red-hat-openshift-application-runtimes/"&gt;Source versus binary S2I workflows with Red Hat OpenShift Application Runtimes&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;RHD Blog&lt;/a&gt;.&lt;/p&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/VNVL4r0yZBg" height="1" width="1" alt=""/&gt;</content><summary type="html">&lt;p&gt;Red Hat OpenShift supports two workflows for building container images for applications: the source and the binary workflows. The binary workflow is the primary focus of the Red Hat OpenShift Application Runtimes and Red Hat Fuse product documentation and training, while the source workflow is the focus of most of the Red Hat OpenShift Container [&amp;#8230;]&lt;/p&gt; &lt;p&gt;The post &lt;a rel="nofollow" href="https://developers.redhat.com/blog/2018/09/26/source-versus-binary-s2i-workflows-with-red-hat-openshift-application-runtimes/"&gt;Source versus binary S2I workflows with Red Hat OpenShift Application Runtimes&lt;/a&gt; appeared first on &lt;a rel="nofollow" href="https://developers.redhat.com/blog"&gt;RHD Blog&lt;/a&gt;.&lt;/p&gt;</summary><wfw:commentRss xmlns:wfw="http://wellformedweb.org/CommentAPI/">https://developers.redhat.com/blog/2018/09/26/source-versus-binary-s2i-workflows-with-red-hat-openshift-application-runtimes/feed/</wfw:commentRss><slash:comments xmlns:slash="http://purl.org/rss/1.0/modules/slash/">0</slash:comments><post-id xmlns="com-wordpress:feed-additions:1">519917</post-id><dc:creator>Fernando Lozano</dc:creator><dc:date>2018-09-26T20:19:11Z</dc:date><feedburner:origLink>https://developers.redhat.com/blog/2018/09/26/source-versus-binary-s2i-workflows-with-red-hat-openshift-application-runtimes/</feedburner:origLink></entry><entry><title>WildFly Elytron - Credential Store - Next Steps</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/xyLFJ8-I5S0/wildfly-elytron-credential-store-next.html" /><category term="credentialstore" scheme="searchisko:content:tags" /><category term="Elytron" scheme="searchisko:content:tags" /><category term="EncryptedData" scheme="searchisko:content:tags" /><category term="feed_group_name_jbossas" scheme="searchisko:content:tags" /><category term="feed_name_darrans_wildfly_blog" scheme="searchisko:content:tags" /><category term="Java" scheme="searchisko:content:tags" /><category term="security" scheme="searchisko:content:tags" /><category term="vault" scheme="searchisko:content:tags" /><category term="wildfly" scheme="searchisko:content:tags" /><author><name>Darran Lofthouse</name></author><id>searchisko:content:id:jbossorg_blog-wildfly_elytron_credential_store_next_steps</id><updated>2018-09-26T12:52:24Z</updated><published>2018-09-26T12:52:00Z</published><content type="html">During the development of WildFly 11 where we introduced the WildFly Elytron project to the application server one of the new features we added was the credential store.&lt;br /&gt;&lt;br /&gt;&lt;a href="https://developers.redhat.com/blog/2017/12/14/new-jboss-eap-7-1-credential-store/"&gt;https://developers.redhat.com/blog/2017/12/14/new-jboss-eap-7-1-credential-store/&lt;/a&gt;&lt;br /&gt;&lt;br /&gt;Now that we are planning the next stages of development for WildFly 15 and 16 we are revisiting some of the next steps for the credential store, this blog post explores some of the history of the current decisions made with the credential store and the types of enhancement being requested to develop this further.&lt;br /&gt;&lt;br /&gt;Anyone making use of either the CredentialStore or Vault is encouraged to provider your feedback so we can take this into account as the next stages are planned.&lt;br /&gt;&lt;br /&gt;&lt;h3&gt;Key Differences To Vault&lt;/h3&gt;Prior to the credential store the PicketBox vault was the solution used for the encryption of credentials and other fields within the application server's configuration, the credential store approach was dedicated to the secure storage of credentials.&lt;br /&gt;&lt;br /&gt;Where the PicketBox vault is used in the application server a single Vault is defined across the whole server and aliases from the vault are referenced via expressions in the server's configuration which allows for the values to be retrieved in clear text.&lt;br /&gt;&lt;br /&gt;The credential store on the other hand allows for multiple credential store instances to be defined, resources that make use of the credentials have been updated to a special attribute type where both the name of the credential store can be specified and the alias of the credential within the credential store.&lt;br /&gt;&lt;br /&gt;The credential store resources additionally support management operations to allow for entries within the credential store to be manipulated either by adding / removing entries or by updating existing entries.&lt;br /&gt;&lt;br /&gt;&lt;h3&gt;Uses of the Store&lt;/h3&gt;Reviewing where the credential store is used we seem to have two predominant scenarios.&lt;br /&gt;&lt;br /&gt;&lt;h4&gt;Unlocking Local Resources&lt;/h4&gt;In this case a credential is required to unlock a local resource such as a KeyStore, there is no remote authentication to be performed and a credential is generally only required for decrypting the contents of the store.&amp;nbsp; This is the simplest use of the store and once the resource is unlocked it is not likely to need unlocking again.&lt;br /&gt;&lt;br /&gt;&lt;h4&gt;Accessing Remote Resources&lt;/h4&gt;The second use we see is for services accessing a remote resource, in this case the credentials for the connection are obtained from the credential store.&lt;br /&gt;&lt;br /&gt;This scenario has a reasonable amount of history also attached to the current implementation, it tended to be the case that if you were to access a remote resource it would either not be secured or it would be secured and authentication would require a username and password.&amp;nbsp; Additionally any SSL related configuration would be handled completely independently.&lt;br /&gt;&lt;br /&gt;In recent years however there has been a greater demand for alternative authentication mechanisms, there has been a lot of demand for Kerberos both with the server being given it's own account for authentication and also for the propagation of a remote user authenticated against the server using Kerberos.&amp;nbsp; We haven't seen requests yet for the application server but I suspect OAuth scenarios will be requested soon.&lt;br /&gt;&lt;br /&gt;In both of these cases the security has moved from username / password authentication to more advanced scenarios.&lt;br /&gt;&lt;br /&gt;In parallel to the credential store WildFly Elytron has also introduced an Authentication Client API, this API can be used for configuring the client side authentication policies for various mechanisms, including scenarios that support propagation of the current identity.&amp;nbsp; The authentication client configuration also allows SSLContext configurations to be associated with a specific destination.&lt;br /&gt;&lt;br /&gt;This now raises the question, should services establishing a remote connection which requires authentication and SSL configuration now reference an authentication client configuration instead of the current assumption that a username and credential reference is sufficient?&lt;br /&gt;&lt;br /&gt;This question becomes quite important as it affects the approach we take to a lot of the enhancements requested of us quite significantly.&lt;br /&gt;&lt;br /&gt;&lt;h3&gt;Next Features&lt;/h3&gt;The features currently being requested against the credential store generally cover three broad areas: -&lt;br /&gt;&lt;br /&gt;&lt;ol&gt;&lt;li&gt;Automation of updates to the store.&lt;/li&gt;&lt;li&gt;Real time updates.&lt;/li&gt;&lt;li&gt;Support for expressions.&lt;/li&gt;&lt;/ol&gt;&lt;br /&gt;&lt;h4&gt;Automation of Updates to the Store&lt;/h4&gt;The general motivation for this enhancement is to simplify the steps configuring resources which require credentials, using the PicketBox Vault implementation the vault would need to be manipulated offline using a command line tool and then references from the application server configuration.&lt;br /&gt;&lt;br /&gt;The CredentialStore has already moved on from this partially as management operations have been exposed to allow the contents of the store to be directly manipulated using management requests so the store can be manipulated directly from the management tools.&amp;nbsp; However an administrator is still required to operate on the two resources completely independently.&lt;br /&gt;&lt;br /&gt;We predominantly have two options to simplify this further.&lt;br /&gt;&lt;br /&gt;We could take the decision that further enhancement is now a tooling issue, the admin clients could detect a resource is being added that supports credential store references or the password on an existing resource that supports credential store references is being set and provide guidance / automation to persist the credential in the credential store.&lt;br /&gt;&lt;br /&gt;&lt;ol&gt;&lt;li&gt;Would you like to store this credential in a credential store?&lt;/li&gt;&lt;li&gt;Which credential store would you like to use? Or would you like to create a new one?&lt;/li&gt;&lt;li&gt;What alias would you like the credential to be stored under?&lt;/li&gt;&lt;/ol&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;Alternatively, the credential reference attribute supports specifying a reference to a credential store and an alias in the credential store - this attribute however also supports specifying a clear text password.&amp;nbsp; We could automate the manipulation in the management tier and if a clear text password is specified on a resource referencing a credential store automatically add it to the credential store and remove it from the model - if no alias is specified automatically generate one.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;We could also support a combination of the two approaches as in the management tier although we could support the interception of a new clear password if a store needs to be created that would be more involved than we could automate.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;h4&gt;Real Time Updates&lt;/h4&gt;&lt;div&gt;Where new credentials are added to a credential store using management operations they are already available for use immediately in the application server process without a restart.&amp;nbsp; However we still have some areas to consider further real time update support: -&lt;/div&gt;&lt;div&gt;&lt;ol&gt;&lt;li&gt;Updates to the store on the filesystem.&lt;/li&gt;&lt;li&gt;Complete replacement of an existing store.&lt;/li&gt;&lt;li&gt;Updates to credentials using management operations.&lt;/li&gt;&lt;/ol&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;/div&gt;&lt;div&gt;In these three cases the primary interest is credentials which are already in use in the application server, however in the case of #1 and #2 it could relate to the addition of new credentials to be used immediately.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;One point to consider is although our default credential store implementation is file based custom implementations could be in use which are not making use of any local files.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;At the credential store level we likely should consider various modes to detect changes when emitting notifications: -&lt;/div&gt;&lt;div&gt;&lt;ol&gt;&lt;li&gt;File system monitoring.&lt;/li&gt;&lt;li&gt;Notifications from within the implementation of the store.&lt;/li&gt;&lt;li&gt;Administrator triggered reloads of either the full store or individual aliases.&lt;/li&gt;&lt;/ol&gt;&lt;/div&gt;&lt;div&gt;At the service level where a service is making use of a credential it is likely we would want to decide how to handle updates on a service by service basis.&amp;nbsp; It is unlikely we would want to automatically restart / replace services as for some services which make use of credentials this could cause a cascade of service restarts potentially leading the redeployment of deployments.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;I expect some form of notifications will be required, at the coarsest level notifications could be emitted for all services accessing a specific credential store - this however could trigger a significant overhead as a single store could contain a large number of entries used across a large number of resources.&amp;nbsp; Instead we could emit notifications just to the resources using the affected aliases, this would be more efficient from the perspective of the notifications but the complexity now is where a coarse update has been updates to a store such as the underlying file being replaced or a full store refresh being triggered by an administrator we now need to identify which credentials were really modified.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;h4&gt;Support for Expressions&lt;/h4&gt;&lt;div&gt;It was a deliberate decision to move away from using expressions, however there are still some demands for expressions that need to be considered.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;Overall the design decisions within WildFly Elytron have always considered a desire to move away from clear text passwords being present in the application server process, where expressions are used the only route available is to obtain the clear text representation of a String and pass it around the related services.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;One of the enhancements delivered for WildFly 11 was to support multiple credential stores concurrently within the application server, by moving to the complex attribute we were able to make use of capability references to select which credential store to use with a second attribute selecting the alias in the store.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;Another consideration was the desire for automatic updates to be applied via the management tier, by moving from expressions to a complex attribute it opens up the options to intercept these values and persist them in the store.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;A further (and possibly the greatest) consideration was the desired support for automatic updates to credentials currently in use in the server, by moving to the capability references aided by the complex attribute definition services can now obtain a direct reference to the store instead of having a credential automatically resolved.&amp;nbsp; By having a reference to a credential store we can potentially add support for direct notifications of updates applied to that store.&amp;nbsp; Where a credential is updated different services may want to respond in different ways, this is why a reference is needed.&amp;nbsp; Within the management tier we can not silently automate the updates, if we were to do so it would likely involve the removal and replacement of the service which could have a side effect of restarting many other services including the deployments.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;The biggest restriction of not supporting expressions is attributes for anything other than a credential can no longer be loaded from the credential store - but the missing piece of information is how that is really used and why?&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;As an example usernames could be loaded using expressions, is this because in some environments the username is being considered as sensitive as the credential?&amp;nbsp; Or is it the case that where a credential is loaded from a store it is easier to load the username from the same store.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;If the answer is co-location of the username and password then a more suitable path to look into may be the externalisation of the authentication client configuration allowing the complete client authentication policy to be handled as a single unit.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;If we are still left with attributes in the configuration that need to be stored securely the next question is do they strictly need to be removed from the configuration and looked up from the store?&amp;nbsp; An alternative option we have to consider is supporting the encryption / decryption of Strings inlined in the management model using a credential from the store.&lt;/div&gt;&lt;br /&gt;&lt;h3&gt;Other Enhancements&lt;/h3&gt;&lt;div&gt;Following on from the main enhancements listed above there is also a set of additional enhancements we could consider.&lt;/div&gt;&lt;h4&gt;Injection / Credential Store Access for Deployments&lt;/h4&gt;&lt;div&gt;Deployments can already access the authentication client configuration, however if access to raw credentials is required it may help to access the store - this could additionally mean a deployment could manipulate the store.&lt;/div&gt;&lt;h4&gt;Permission Checks&lt;/h4&gt;&lt;div&gt;Once accessed within deployments, making use of the SecurityIdentity permission checks we could perform permission checks for different read / write operations on the credential store.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;h4&gt;Auditing&lt;/h4&gt;&lt;div&gt;The credential store could be updated to emit security events as it is accessed, by emitting security events these can be output to audit logs or sent to other event analysing frameworks.&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/xyLFJ8-I5S0" height="1" width="1" alt=""/&gt;</content><summary>During the development of WildFly 11 where we introduced the WildFly Elytron project to the application server one of the new features we added was the credential store. https://developers.redhat.com/blog/2017/12/14/new-jboss-eap-7-1-credential-store/ Now that we are planning the next stages of development for WildFly 15 and 16 we are revisiting some of the next steps for the credential store, thi...</summary><dc:creator>Darran Lofthouse</dc:creator><dc:date>2018-09-26T12:52:00Z</dc:date><feedburner:origLink>http://darranl.blogspot.com/2018/09/wildfly-elytron-credential-store-next.html</feedburner:origLink></entry><entry><title>Colloquium - Making sense of enterprise open source software</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/7T6gELFeBdw/colloquium-making-sense-of-enterprise.html" /><category term="conference" scheme="searchisko:content:tags" /><category term="english" scheme="searchisko:content:tags" /><category term="feed_group_name_jbossas" scheme="searchisko:content:tags" /><category term="feed_name_dimitris" scheme="searchisko:content:tags" /><category term="JBoss" scheme="searchisko:content:tags" /><category term="opensource" scheme="searchisko:content:tags" /><category term="redhat" scheme="searchisko:content:tags" /><category term="switzerland" scheme="searchisko:content:tags" /><author><name>Dimitris Andreadis</name></author><id>searchisko:content:id:jbossorg_blog-colloquium_making_sense_of_enterprise_open_source_software</id><updated>2018-09-26T12:40:25Z</updated><published>2018-09-26T12:40:25Z</published><content type="html">&lt;div dir="ltr" style="text-align: left;" trbidi="on"&gt;The coming Friday, Sep/28th @ 2pm, I have the pleasure to be talking at the &lt;a href="https://www3.unifr.ch/inf/en/"&gt;Department of Informatics of the University of Fribourg&lt;/a&gt; on the subject of: &lt;a href="http://mcs.unibnf.ch/events/informatics-colloquium-39"&gt;"Making sense of enterprise open source software"&lt;/a&gt;.&lt;br /&gt;&lt;br /&gt;Copying here the Abstract from the event &lt;a href="http://mcs.unibnf.ch/sites/diuf.unifr.ch.drupal.mcs/files/downloads/Affiche%20Andreadis.pdf"&gt;flyer&lt;/a&gt;:&lt;br /&gt;&lt;blockquote class="tr_bq"&gt;Red Hat is a leading enterprise software provider that has built a business model around something that is perceived as "free": open source software. In fact, last year Red Hat managed to sell about $3 billion dollars of "free" software and services to the likes of Fortune 500 companies. How can this be possible? How does an open source business model work in practice? Where does it make sense? Why open source has prevailed in so many different technology domains?&lt;br /&gt;&lt;br /&gt;Come to this talk to discover the nuances of enterprise open source software seen from the point of view of JBoss, a popular open source application server project and a start-up company built around it that was acquired by Red Hat back in 2006 to form Red Hat's Middleware division. Also, learn the secrets of how one becomes a successful open source software developer, should you want to get involved with the open source movement, build a career out of it and have a lot of fun on the way.&lt;/blockquote&gt;The event is hosted by &lt;a href="https://www.linkedin.com/in/p-c-m/"&gt;Prof. Philippe Cudré-Mauroux&lt;/a&gt;, whom I'd like to thank for the invitation. It is also perfectly timed so you can be back in Neuchâtel on time for the start of the &lt;a href="http://www.fete-des-vendanges.ch/"&gt;Fête des Vendages&lt;/a&gt;. :)&lt;br /&gt;&lt;br /&gt;See you &lt;a href="https://goo.gl/maps/WipueANDx562"&gt;there&lt;/a&gt;!&lt;br /&gt;&lt;br /&gt;/&lt;a href="http://dandreadis.blogspot.com/"&gt;Dimitris&lt;/a&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;/div&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/7T6gELFeBdw" height="1" width="1" alt=""/&gt;</content><summary>The coming Friday, Sep/28th @ 2pm, I have the pleasure to be talking at the Department of Informatics of the University of Fribourg on the subject of: "Making sense of enterprise open source software". Copying here the Abstract from the event flyer: Red Hat is a leading enterprise software provider that has built a business model around something that is perceived as "free": open source software. ...</summary><dc:creator>Dimitris Andreadis</dc:creator><dc:date>2018-09-26T12:40:25Z</dc:date><feedburner:origLink>http://dandreadis.blogspot.com/2018/09/colloquium-making-sense-of-enterprise.html</feedburner:origLink></entry><entry><title>Business Applications by jBPM - League of Legends Stats Demo</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/12V-VHvjFg8/business-applications-by-jbpm-league-of.html" /><category term="feed_group_name_jbossjbpmcommunity" scheme="searchisko:content:tags" /><category term="feed_name_swiderskimaciej" scheme="searchisko:content:tags" /><author><name>Tihomir Surdilovic</name></author><id>searchisko:content:id:jbossorg_blog-business_applications_by_jbpm_league_of_legends_stats_demo</id><updated>2018-09-25T14:32:42Z</updated><published>2018-09-25T14:28:00Z</published><content type="html">A lot of our development focus recently has been around business applications, specifically rapid creation/development of business apps that include the full power of jBPM, are easily deployable to the cloud, and are fun and easy to work with.&lt;br /&gt;We are at a point currently where we can start showcasing our work and what better way than with a demo.&lt;br /&gt;&lt;br /&gt;This demo shows off the power of business app generation with jBPM and shows how easy it is to then extend it to create a unique and fun app for your users/customers.&lt;br /&gt;&lt;br /&gt;&lt;br /&gt;&lt;b&gt;The youtube video for the demo can be found &lt;a href="https://www.youtube.com/watch?v=W-gtooRgOTw"&gt;here&lt;/a&gt;&amp;nbsp;(or click on the image below).&lt;/b&gt;&lt;br /&gt;&lt;div class="separator" style="clear: both; text-align: center;"&gt;&lt;a href="https://www.youtube.com/watch?v=W-gtooRgOTw&amp;amp;t=166s"&gt;&lt;img border="0" data-original-height="1022" data-original-width="1600" height="203" src="https://1.bp.blogspot.com/-wGRwxAc_4OM/W6pGjtscqvI/AAAAAAAAhVE/uLPfFomOe_QaoEw1tEB9-MAj1tSsIZnrQCLcBGAs/s320/Screen%2BShot%2B2018-09-25%2Bat%2B10.41.54%2BAM.png" width="320" /&gt;&lt;/a&gt;&lt;span id="goog_1583922302"&gt;&lt;/span&gt;&lt;a href="https://www.blogger.com/"&gt;&lt;/a&gt;&lt;span id="goog_1583922303"&gt;&lt;/span&gt;&lt;/div&gt;&lt;br /&gt;Feel free to leave any comments/questions/ideas on the video itself or here if you like. The video shows how we generated the business app with start.jbpm.org, extend it and gives alot of good info on how to get up and running with all this.&lt;br /&gt;&lt;br /&gt;And also here are some important links to get you started:&lt;br /&gt;&lt;ul&gt;&lt;li&gt;Generate your business app - &lt;a href="http://start.jbpm.org/"&gt;start.jbpm.org&lt;/a&gt;&lt;/li&gt;&lt;li&gt;Read more about business apps and jBPM in &lt;a href="https://docs.jboss.org/jbpm/release/7.11.0.Final/jbpm-docs/html_single/index.html#_businessappoverview"&gt;docs&lt;/a&gt;.&lt;/li&gt;&lt;li&gt;Demo app &lt;a href="https://github.com/business-applications/sample-riot-league-stats"&gt;source code and install instructions&lt;/a&gt;.&lt;/li&gt;&lt;/ul&gt;And some images of the demo app that you can build easily yourself:&lt;br /&gt;&lt;br /&gt;&lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto; text-align: center;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://1.bp.blogspot.com/-u8aIlGABnA8/W6pEb-vhTPI/AAAAAAAAhU4/EY8pQCghSSY7NViiF62wZQoCsvM9AEPUQCLcBGAs/s1600/Screen%2BShot%2B2018-09-25%2Bat%2B10.33.31%2BAM.png" imageanchor="1" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="930" data-original-width="1600" height="186" src="https://1.bp.blogspot.com/-u8aIlGABnA8/W6pEb-vhTPI/AAAAAAAAhU4/EY8pQCghSSY7NViiF62wZQoCsvM9AEPUQCLcBGAs/s320/Screen%2BShot%2B2018-09-25%2Bat%2B10.33.31%2BAM.png" width="320" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;Demo app - Summoner Search&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;br /&gt;&lt;br /&gt;&lt;table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto; text-align: center;"&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td style="text-align: center;"&gt;&lt;a href="https://2.bp.blogspot.com/-pIwFY6Xqidw/W6pD_m4CTBI/AAAAAAAAhUw/g02xa2kgowQuObuSmqYRJF8MKKN4PguhQCLcBGAs/s1600/Screen%2BShot%2B2018-09-25%2Bat%2B10.00.27%2BAM.png" imageanchor="1" style="margin-left: auto; margin-right: auto;"&gt;&lt;img border="0" data-original-height="1100" data-original-width="1600" height="219" src="https://2.bp.blogspot.com/-pIwFY6Xqidw/W6pD_m4CTBI/AAAAAAAAhUw/g02xa2kgowQuObuSmqYRJF8MKKN4PguhQCLcBGAs/s320/Screen%2BShot%2B2018-09-25%2Bat%2B10.00.27%2BAM.png" width="320" /&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;&lt;tr&gt;&lt;td class="tr-caption" style="text-align: center;"&gt;Demo app - Match Results&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;br /&gt;Let us know what you think. A lot more info about this is coming your way so stay tuned and have fun generating your business apps!&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/12V-VHvjFg8" height="1" width="1" alt=""/&gt;</content><summary>A lot of our development focus recently has been around business applications, specifically rapid creation/development of business apps that include the full power of jBPM, are easily deployable to the cloud, and are fun and easy to work with. We are at a point currently where we can start showcasing our work and what better way than with a demo. This demo shows off the power of business app gener...</summary><dc:creator>Tihomir Surdilovic</dc:creator><dc:date>2018-09-25T14:28:00Z</dc:date><feedburner:origLink>http://mswiderski.blogspot.com/2018/09/business-applications-by-jbpm-league-of.html</feedburner:origLink></entry></feed>
